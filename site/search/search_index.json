{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deploying Apereo CAS Deploying Apereo CAS provides step-by-step instructions for setting up Apereo CAS 6 with a goal of deploying and maintaining both CAS and the application server (Tomcat) via Ansible. The goal of this is to make deploying and maintaining CAS and Tomcat easy. The idea for creating this (and some of the content) is based on the amazing Deploying Apereo CAS documentation created for CAS 5, created by David Curry of The New School. It is meant to supplement, not replace, the documentation created by Apereo for CAS. This is my first foray into using Github pages, or making documentation for deploying an application like this (my documentation has either been end user facing or for internal staff up until now) - so apologies for any rough edges on it. It's incomplete as of now but my goal is to document as I deploy into test and later into production. It should be mostly complete by the end of March 2021. It was created in my work at the State University of New York at New Paltz but I wanted to give back to the CAS community for the great information I've found from so many. Copyright and Licensing Documentation Content All documentation content is Copyright \u00a9 2021, SUNY New Paltz. It is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License . Site Template This site was created with MkDocs using the Material Theme . Author Information Paul Chauvet Information Security Officer State University of New York at New Paltz chauvetp@newpaltz.edu DISCLAIMER The instructions and settings provided in this document may not be the only way to do things. They are the way that has worked for us at New Paltz, and I've tried to document them as well as possible - but there may be better/cleaner ways of doing things. They may not work at all for your environment. Heck - I may have made some big mistakes here. As always - test test test. You should not go running into this on a production environment without ample testing AND understanding of the setup (both Tomcat, and CAS). No warranty express or implied. No support guaranteed. If you use this - and find it useful - let me know! If you use it and find errors or would suggest changes - sure - let me know those too! If you have suggestions as to how to do this (insert some completely different way) - you may want to fork this or create your own site from scratch for it.","title":"Main"},{"location":"#deploying-apereo-cas","text":"Deploying Apereo CAS provides step-by-step instructions for setting up Apereo CAS 6 with a goal of deploying and maintaining both CAS and the application server (Tomcat) via Ansible. The goal of this is to make deploying and maintaining CAS and Tomcat easy. The idea for creating this (and some of the content) is based on the amazing Deploying Apereo CAS documentation created for CAS 5, created by David Curry of The New School. It is meant to supplement, not replace, the documentation created by Apereo for CAS. This is my first foray into using Github pages, or making documentation for deploying an application like this (my documentation has either been end user facing or for internal staff up until now) - so apologies for any rough edges on it. It's incomplete as of now but my goal is to document as I deploy into test and later into production. It should be mostly complete by the end of March 2021. It was created in my work at the State University of New York at New Paltz but I wanted to give back to the CAS community for the great information I've found from so many.","title":"Deploying Apereo CAS"},{"location":"#copyright-and-licensing","text":"","title":"Copyright and Licensing"},{"location":"#documentation-content","text":"All documentation content is Copyright \u00a9 2021, SUNY New Paltz. It is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License .","title":"Documentation Content"},{"location":"#site-template","text":"This site was created with MkDocs using the Material Theme .","title":"Site Template"},{"location":"#author-information","text":"Paul Chauvet Information Security Officer State University of New York at New Paltz chauvetp@newpaltz.edu DISCLAIMER The instructions and settings provided in this document may not be the only way to do things. They are the way that has worked for us at New Paltz, and I've tried to document them as well as possible - but there may be better/cleaner ways of doing things. They may not work at all for your environment. Heck - I may have made some big mistakes here. As always - test test test. You should not go running into this on a production environment without ample testing AND understanding of the setup (both Tomcat, and CAS). No warranty express or implied. No support guaranteed. If you use this - and find it useful - let me know! If you use it and find errors or would suggest changes - sure - let me know those too! If you have suggestions as to how to do this (insert some completely different way) - you may want to fork this or create your own site from scratch for it.","title":"Author Information"},{"location":"about/about-new-paltz/","text":"About the State University of New York at New Paltz What is now the State University of New York at New Paltz has existed in some form in New Paltz, NY since 1828. It was originally the New Paltz Classic school, later a state normal school that trained teahcers for New York State public schools, and an official four-year college in 1938 as the State Teachers College at New Paltz. It was one of the founding schools of the SUNY system in 1948. Today, the State University of New York at New Paltz covers 216 acres and includes the College of Liberal Arts & Sciences, the School of Business, the School of Science & Engineering, the School of Fine & Performing Arts, the School of Education, and The Graduate School. Over time, the curricular mission of the school has broadened from the nearly single-track of the Normal School to 143 programs in six schools. For more information, visit www.newpaltz.edu .","title":"About SUNY New Paltz"},{"location":"about/about-new-paltz/#about-the-state-university-of-new-york-at-new-paltz","text":"What is now the State University of New York at New Paltz has existed in some form in New Paltz, NY since 1828. It was originally the New Paltz Classic school, later a state normal school that trained teahcers for New York State public schools, and an official four-year college in 1938 as the State Teachers College at New Paltz. It was one of the founding schools of the SUNY system in 1948. Today, the State University of New York at New Paltz covers 216 acres and includes the College of Liberal Arts & Sciences, the School of Business, the School of Science & Engineering, the School of Fine & Performing Arts, the School of Education, and The Graduate School. Over time, the curricular mission of the school has broadened from the nearly single-track of the Normal School to 143 programs in six schools. For more information, visit www.newpaltz.edu .","title":"About the State University of New York at New Paltz"},{"location":"about/architecture/","text":"System/Architecture Overview Components This document was created to reflect the environment in use at New Paltz for CAS. Apereo CAS 6.2.x Red Hat Enterprise Linux 8 Apache Tomcat 9.0.x Apache httpd Ansible Azure Active Directory Microsoft Active Directory Duo for Multifactor authentication Hazelcast Ticket Registry for ticket storage between systems. If you want to use another version of CAS 6.x - it may or may not have significant changes. I'll be moving from CAS 6.2 to 6.3 and will document any issues as I come across them, at least in our environment. If you are going to use another operating system, web server, or java servlet container, I'd imagine the CAS portion of the instructions will be relatively similar - though the Ansible deployments for Tomcat may be less similar or useful. Design We have three levels of CAS here at New Paltz, Production, Test, and Development. Test is used for non-production applications that still need SSO (for example Banner test). Development is exclusively used for building, and testing, new versions or configuration changes for CAS. Authentication will be against either: On-prem Active Directory (for alumni) Azure Active Directory (for active faculty/staff/students, and retirees) (we may eventually have all alumni in Azure - but that's for a later date. For now - they are only on-prem and really are only kept active for a couple systems) In each case - the hosts sit behind a load balancer (in our case, F5 Big IP, though HA Proxy or basically any other load balancer should work. We're not doing anything crazy at the LB level). The load balancer is split in multiple locations, as are the application servers. This gives us resilience if a single site on-campus is down but does NOT give resilience against a full outage on-campus. This is one reason why we are looking to have as many services as possible authenticate directly against Azure instead of CAS. Using CAS to delegate authentication to Azure, we can make this transition as streamlined as possible without having people enter login credentials more than once. This will also let us make our transition of existing CAS apps to SAML gradually instead of having to deal with a ton of internal and external service providers all at once.","title":"Architecture/design"},{"location":"about/architecture/#systemarchitecture-overview","text":"","title":"System/Architecture Overview"},{"location":"about/architecture/#components","text":"This document was created to reflect the environment in use at New Paltz for CAS. Apereo CAS 6.2.x Red Hat Enterprise Linux 8 Apache Tomcat 9.0.x Apache httpd Ansible Azure Active Directory Microsoft Active Directory Duo for Multifactor authentication Hazelcast Ticket Registry for ticket storage between systems. If you want to use another version of CAS 6.x - it may or may not have significant changes. I'll be moving from CAS 6.2 to 6.3 and will document any issues as I come across them, at least in our environment. If you are going to use another operating system, web server, or java servlet container, I'd imagine the CAS portion of the instructions will be relatively similar - though the Ansible deployments for Tomcat may be less similar or useful.","title":"Components"},{"location":"about/architecture/#design","text":"We have three levels of CAS here at New Paltz, Production, Test, and Development. Test is used for non-production applications that still need SSO (for example Banner test). Development is exclusively used for building, and testing, new versions or configuration changes for CAS. Authentication will be against either: On-prem Active Directory (for alumni) Azure Active Directory (for active faculty/staff/students, and retirees) (we may eventually have all alumni in Azure - but that's for a later date. For now - they are only on-prem and really are only kept active for a couple systems) In each case - the hosts sit behind a load balancer (in our case, F5 Big IP, though HA Proxy or basically any other load balancer should work. We're not doing anything crazy at the LB level). The load balancer is split in multiple locations, as are the application servers. This gives us resilience if a single site on-campus is down but does NOT give resilience against a full outage on-campus. This is one reason why we are looking to have as many services as possible authenticate directly against Azure instead of CAS. Using CAS to delegate authentication to Azure, we can make this transition as streamlined as possible without having people enter login credentials more than once. This will also let us make our transition of existing CAS apps to SAML gradually instead of having to deal with a ton of internal and external service providers all at once.","title":"Design"},{"location":"about/author-information/","text":"This documentation was created by: Paul Chauvet, CISSP Information Security Officer State University of New York at New Paltz chauvetp@newpaltz.edu Who am I? Well, I'm currently the Information Security Officer in Information Technology Services at SUNY New Paltz, where I've worked since 2003. Since 2019, I've also been teaching Cybersecurity as an adjunct in the Computer Science department here. I've been at New Paltz a long time... I graduated in 2001 with a B.S. in Computer Engineering, and went to work in NYC for a couple years before returning back to New Paltz as an employee in late 2003. While working at the college, I received my MBA in 2011. I've been working full time in Information Security at the college since 2014. When not at work - you may see me at one of the beautiful hiking areas in the Hudson Valley. I'd like to thank all those who contributed to this - whether via their public documentation, forum posts on the Apereo CAS forums, or directly via email. This includes David Curry of The New School , David Warden and Shawn Plummer of SUNY Geneseo , Ken Runyon of SUNY System Administration , Matt Adkins of Liberty University , David Campman of Forsyte IT , and all the developers who have contributed to the Apereo CAS project and who contribute in the CAS Community . The organization (and not a small amount of the content, especially the base Tomcat configuration guidelines) is straight from David Curry's Deploying Apereo CAS 5 site - so special thanks go to him. I used his documentation as a starting point when we first moved from CAS 4.x to 5.x a few years ago. Part of the reason I am creating this document is so that others can (hopefully!) get as much benefit from this document for CAS 6 as I did with his for CAS 5.","title":"About the author"},{"location":"building-cas/ansible-cas/","text":"Setup variables If you setup the four variables we needed in the encrypted vault file, you're ready. Those variables are setup like the following (the actual valus shown here are not those in use in my environment): dev_tgc_signing_key : dYffipMGbhIoyUsSHwzEDcBk5nbETZtH-lR3R776wNavS4koAHyQkDdK_rJIWrYYgZZ2TsLW5NXfcDI_Ivn4Uw dev_tgc_encryption_key : Gpy6fxMnh5RhmFUJWVcJL9WIAuFODzTlPIaQOTq9-jM dev_webflow_signing_key : xoWJ9S2vmTgvB_CdZeddb1qmqPihBGIw5Op27MsNxfR8KWgPgrx4VXpssTTM3IcXkLJVoxTylg_hSxvH65M88g dev_webflow_encryption_key : qnHUX0kyFHc718oh/f+ebw== Setup templates To start - the only template files we need are: dev-cas.properties.j2 (which will be /etc/cas/config/cas.properties on the Dev CAS systems) log4j2.xml (which will be /etc/cas/config/log4j2.xml on the CAS systems, as so far I haven't had a reason to have a different log4j2 config on DEV/TEST/PROD like I have for cas.properties). Setup handlers You can copy over the handlers/main.yml file from the Apache Tomcat role since you'll need it here. Contents are the same, just: # handlers file for apache-tomcat # All that's really here for now is just commands to start, stop, and restart tomcat. - name : stop tomcat ansible.builtin.systemd : name : tomcat state : stopped - name : start tomcat ansible.builtin.systemd : name : tomcat state : started - name : restart tomcat ansible.builtin.systemd : name : tomcat state : restarted Create tasks We're going to have multiple plays within the CAS6 role, so we'll break them up like we did for Tomcat, but to start with, it's just a single include for 'base-cas-config.yml'. The tasks are: Make sure the cas config and services directories exist Make sure that the cas.properties file we have in our templates directory matches the one in /etc/cas/config (and if not - update it and use notify to restart Tomcat when the play is done). Make sure that the log4j2.xml file we have in our files directory matches the one in /etc/cas/config (and if not - update it and use notify to restart Tomcat when the play is done). Make sure that the cas.war file we have in our files directory matches the one in /opt/tomcat/latest/webapps (and if not - update it and use notify to restart Tomcat when the play is done). - include_tasks : base-cas-config.yml within base-cas-config.yml: --- - include_vars : cas-vault.yml - name : Ensure base CAS config directory exists file : path : /etc/cas/config state : directory mode : 750 owner : root group : tomcat - name : Ensure base CAS services directory exists file : path : /etc/cas/services state : directory mode : 750 owner : root group : tomcat # Note: This is for dev specifically. If we have multiple environments, there's # a different config file for each. The 'when' on inventory_hostname is used to # differentiate here. - name : Configure cas.properties file (dev) template : src : dev-cas.properties.j2 dest : /etc/cas/config/cas.properties mode : 0640 owner : root group : tomcat when : (\"login6dev\" in inventory_hostname) notify : restart tomcat # For us at least, log4j2 is the same on production, dev, or test, # so it's not tier dependent and doesn't need the 'when' statement. # This uses the 'copy' module instead of the 'template' module since # Ansible does not like the {} all over that file. If you need to change that per-server # You'll need to create an version of log4j2.xml with {} escaped. - name : Copy log4j2.xml ansible.builtin.copy : src : log4j2.xml dest : /etc/cas/config/log4j2.xml mode : 0640 owner : root group : tomcat notify : restart tomcat # Note: cas6.war file should be placed in the 'files' subdirectory of your cas role. # You can explicitly replace 'src' with the path to it, i.e. /home/your-user/cas-overlay-template/build/libs/cas.war if you # want, or you can manually copy the cas.war file into the files directory. - name : Copy CAS war file ansible.builtin.copy : src : cas.war dest : /opt/tomcat/latest/webapps/cas.war mode : 0750 owner : root group : tomcat when : (\"login6dev\" in inventory_hostname) notify : restart tomcat Run the play In this example below, I'm running the play but limiting it to only one host. [ chauvetp@ansible templates ] $ ansible-playbook ~/ansible/site.yml --ask-vault-pass --limit login6devb Vault password : This will cause the dev-cas.properties.j2 template to be copied over to /etc/cas/config/cas.properties. It will also copy the log4j2.xml file from the files directory on the ansible host to /etc/cas/config/log4j2.xml on the system or systems specified in the limit command. It will also substitute the variables in the cas properties file using the contents of cas-vault.yml, which were decrypted via the --ask-vault-pass option. If the cas.properties file on the target system is different than the template (after variable substitution), then it will also restart Tomcat. Test the install Once your play goes through - you will want to go to your server (i.e. https://YourCASServer.domain.edu/cas/login) and check that it loads. If you left in a test user defined by cas.authn.accept.users in cas.properties, you can test those credentials here. If it succeeds, you'll receive a \"Log In Successful\" message.","title":"Ansible playbook setup"},{"location":"building-cas/ansible-cas/#setup-variables","text":"If you setup the four variables we needed in the encrypted vault file, you're ready. Those variables are setup like the following (the actual valus shown here are not those in use in my environment): dev_tgc_signing_key : dYffipMGbhIoyUsSHwzEDcBk5nbETZtH-lR3R776wNavS4koAHyQkDdK_rJIWrYYgZZ2TsLW5NXfcDI_Ivn4Uw dev_tgc_encryption_key : Gpy6fxMnh5RhmFUJWVcJL9WIAuFODzTlPIaQOTq9-jM dev_webflow_signing_key : xoWJ9S2vmTgvB_CdZeddb1qmqPihBGIw5Op27MsNxfR8KWgPgrx4VXpssTTM3IcXkLJVoxTylg_hSxvH65M88g dev_webflow_encryption_key : qnHUX0kyFHc718oh/f+ebw==","title":"Setup variables"},{"location":"building-cas/ansible-cas/#setup-templates","text":"To start - the only template files we need are: dev-cas.properties.j2 (which will be /etc/cas/config/cas.properties on the Dev CAS systems) log4j2.xml (which will be /etc/cas/config/log4j2.xml on the CAS systems, as so far I haven't had a reason to have a different log4j2 config on DEV/TEST/PROD like I have for cas.properties).","title":"Setup templates"},{"location":"building-cas/ansible-cas/#setup-handlers","text":"You can copy over the handlers/main.yml file from the Apache Tomcat role since you'll need it here. Contents are the same, just: # handlers file for apache-tomcat # All that's really here for now is just commands to start, stop, and restart tomcat. - name : stop tomcat ansible.builtin.systemd : name : tomcat state : stopped - name : start tomcat ansible.builtin.systemd : name : tomcat state : started - name : restart tomcat ansible.builtin.systemd : name : tomcat state : restarted","title":"Setup handlers"},{"location":"building-cas/ansible-cas/#create-tasks","text":"We're going to have multiple plays within the CAS6 role, so we'll break them up like we did for Tomcat, but to start with, it's just a single include for 'base-cas-config.yml'. The tasks are: Make sure the cas config and services directories exist Make sure that the cas.properties file we have in our templates directory matches the one in /etc/cas/config (and if not - update it and use notify to restart Tomcat when the play is done). Make sure that the log4j2.xml file we have in our files directory matches the one in /etc/cas/config (and if not - update it and use notify to restart Tomcat when the play is done). Make sure that the cas.war file we have in our files directory matches the one in /opt/tomcat/latest/webapps (and if not - update it and use notify to restart Tomcat when the play is done). - include_tasks : base-cas-config.yml within base-cas-config.yml: --- - include_vars : cas-vault.yml - name : Ensure base CAS config directory exists file : path : /etc/cas/config state : directory mode : 750 owner : root group : tomcat - name : Ensure base CAS services directory exists file : path : /etc/cas/services state : directory mode : 750 owner : root group : tomcat # Note: This is for dev specifically. If we have multiple environments, there's # a different config file for each. The 'when' on inventory_hostname is used to # differentiate here. - name : Configure cas.properties file (dev) template : src : dev-cas.properties.j2 dest : /etc/cas/config/cas.properties mode : 0640 owner : root group : tomcat when : (\"login6dev\" in inventory_hostname) notify : restart tomcat # For us at least, log4j2 is the same on production, dev, or test, # so it's not tier dependent and doesn't need the 'when' statement. # This uses the 'copy' module instead of the 'template' module since # Ansible does not like the {} all over that file. If you need to change that per-server # You'll need to create an version of log4j2.xml with {} escaped. - name : Copy log4j2.xml ansible.builtin.copy : src : log4j2.xml dest : /etc/cas/config/log4j2.xml mode : 0640 owner : root group : tomcat notify : restart tomcat # Note: cas6.war file should be placed in the 'files' subdirectory of your cas role. # You can explicitly replace 'src' with the path to it, i.e. /home/your-user/cas-overlay-template/build/libs/cas.war if you # want, or you can manually copy the cas.war file into the files directory. - name : Copy CAS war file ansible.builtin.copy : src : cas.war dest : /opt/tomcat/latest/webapps/cas.war mode : 0750 owner : root group : tomcat when : (\"login6dev\" in inventory_hostname) notify : restart tomcat","title":"Create tasks"},{"location":"building-cas/ansible-cas/#run-the-play","text":"In this example below, I'm running the play but limiting it to only one host. [ chauvetp@ansible templates ] $ ansible-playbook ~/ansible/site.yml --ask-vault-pass --limit login6devb Vault password : This will cause the dev-cas.properties.j2 template to be copied over to /etc/cas/config/cas.properties. It will also copy the log4j2.xml file from the files directory on the ansible host to /etc/cas/config/log4j2.xml on the system or systems specified in the limit command. It will also substitute the variables in the cas properties file using the contents of cas-vault.yml, which were decrypted via the --ask-vault-pass option. If the cas.properties file on the target system is different than the template (after variable substitution), then it will also restart Tomcat.","title":"Run the play"},{"location":"building-cas/ansible-cas/#test-the-install","text":"Once your play goes through - you will want to go to your server (i.e. https://YourCASServer.domain.edu/cas/login) and check that it loads. If you left in a test user defined by cas.authn.accept.users in cas.properties, you can test those credentials here. If it succeeds, you'll receive a \"Log In Successful\" message.","title":"Test the install"},{"location":"building-cas/cas-log-config/","text":"Configure CAS logging The Log4J configuration file included with the Gradle WAR overlay template will attempt to write the CAS server log files to /var/log. This is an improvement from CAS 5 where it wanted to default to using the Tomcat directory, but I still prefer to have CAS logs in a separate directory of /var/log/cas. I'd also recommend altering the log rotations so that you have a single file each day instead of it being split by size. These steps are recommended but not required. Change CAS log directory (optional) Make a copy of the file etc/cas/config/log4j2.xml in the cas-overlay-template directory from your build server, and copy it into the files subdirectory of your CAS Ansible role. Find the line that defines the cas.log.dir property (around line 5) and change its value from /var/log to /var/log/cas. Initial config: <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!-- Specify the refresh internal in seconds. --> <Configuration monitorInterval= \"5\" packages= \"org.apereo.cas.logging\" > <Properties> <Property name= \"baseDir\" > /var/log </Property> <Property name= \"cas.log.level\" > info </Property> Altered config with /var/log/cas as the directory: <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!-- Specify the refresh internal in seconds. --> <Configuration monitorInterval= \"5\" packages= \"org.apereo.cas.logging\" > <Properties> <Property name= \"baseDir\" > /var/log/cas </Property> <Property name= \"cas.log.level\" > info </Property> You'll also need to ensure that /var/log/cas exists, and is owned by tomcat (though you can skip this as our Ansible playbook will handle this): mkdir /var/log/cas chown tomcat:tomcat /var/log/cas chmod 750 /var/log/cas Alter the log file rotation strategy (optional) Look for the RollingFile configuration for cas.log (around line 23), and change the variable part of the filePattern attribute to remove the -%i portion. This is the hour and sequence number, which is unneeded if we're only at one file per day. filePattern=\"${baseDir}/cas-%d{yyyy-MM-dd-HH} -%i .log\"> filePattern=\"${baseDir}/cas-%d{yyyy-MM-dd-HH}.log\">. Remove (or comment out) the OnStartupTriggeringPolicy element (around line 27). Remove (or comment out) the SizeBasedTriggeringPolicy element (around line 28). Add the attributes interval=\"1\" modulate=\"true\" to the TimeBasedTriggeringPolicy element (around line 29). Repeat steps 1-4, but for the cas_audit.log config - right after the cas.log config. When done - the cas.log and cas_audit.log sections of log4j2.xml will look like the following: <RollingFile name= \"file\" fileName= \"${baseDir}/cas.log\" append= \"true\" filePattern= \"${baseDir}/cas-%d{yyyy-MM-dd-HH}-%i.log\" > <PatternLayout pattern= \"%d %p [%c] - &lt;%m&gt;%n\" /> <Policies> <!-- Not using OnStartupTriggering or SizeBasedTriggering Policies <OnStartupTriggeringPolicy /> <SizeBasedTriggeringPolicy size=\"10 MB\"/> --> <TimeBasedTriggeringPolicy interval= \"1\" module= \"true\" /> </Policies> </RollingFile> <RollingFile name= \"auditlogfile\" fileName= \"${baseDir}/cas_audit.log\" append= \"true\" filePattern= \"${baseDir}/cas_audit-%d{yyyy-MM-dd-HH}.log\" > <PatternLayout pattern= \"%d %p [%c] - %m%n\" /> <Policies> <!-- Not using OnStartupTriggering or SizeBasedTriggering Policies <OnStartupTriggeringPolicy /> <SizeBasedTriggeringPolicy size=\"10 MB\"/> --> <TimeBasedTriggeringPolicy interval= \"1\" module= \"true\" /> </Policies> </RollingFile> These changes have been incorporated into our ansible playbook - as you'll see in the next section.","title":"Configure CAS logs"},{"location":"building-cas/cas-log-config/#configure-cas-logging","text":"The Log4J configuration file included with the Gradle WAR overlay template will attempt to write the CAS server log files to /var/log. This is an improvement from CAS 5 where it wanted to default to using the Tomcat directory, but I still prefer to have CAS logs in a separate directory of /var/log/cas. I'd also recommend altering the log rotations so that you have a single file each day instead of it being split by size. These steps are recommended but not required.","title":"Configure CAS logging"},{"location":"building-cas/cas-log-config/#change-cas-log-directory-optional","text":"Make a copy of the file etc/cas/config/log4j2.xml in the cas-overlay-template directory from your build server, and copy it into the files subdirectory of your CAS Ansible role. Find the line that defines the cas.log.dir property (around line 5) and change its value from /var/log to /var/log/cas. Initial config: <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!-- Specify the refresh internal in seconds. --> <Configuration monitorInterval= \"5\" packages= \"org.apereo.cas.logging\" > <Properties> <Property name= \"baseDir\" > /var/log </Property> <Property name= \"cas.log.level\" > info </Property> Altered config with /var/log/cas as the directory: <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!-- Specify the refresh internal in seconds. --> <Configuration monitorInterval= \"5\" packages= \"org.apereo.cas.logging\" > <Properties> <Property name= \"baseDir\" > /var/log/cas </Property> <Property name= \"cas.log.level\" > info </Property> You'll also need to ensure that /var/log/cas exists, and is owned by tomcat (though you can skip this as our Ansible playbook will handle this): mkdir /var/log/cas chown tomcat:tomcat /var/log/cas chmod 750 /var/log/cas","title":"Change CAS log directory (optional)"},{"location":"building-cas/cas-log-config/#alter-the-log-file-rotation-strategy-optional","text":"Look for the RollingFile configuration for cas.log (around line 23), and change the variable part of the filePattern attribute to remove the -%i portion. This is the hour and sequence number, which is unneeded if we're only at one file per day. filePattern=\"${baseDir}/cas-%d{yyyy-MM-dd-HH} -%i .log\"> filePattern=\"${baseDir}/cas-%d{yyyy-MM-dd-HH}.log\">. Remove (or comment out) the OnStartupTriggeringPolicy element (around line 27). Remove (or comment out) the SizeBasedTriggeringPolicy element (around line 28). Add the attributes interval=\"1\" modulate=\"true\" to the TimeBasedTriggeringPolicy element (around line 29). Repeat steps 1-4, but for the cas_audit.log config - right after the cas.log config. When done - the cas.log and cas_audit.log sections of log4j2.xml will look like the following: <RollingFile name= \"file\" fileName= \"${baseDir}/cas.log\" append= \"true\" filePattern= \"${baseDir}/cas-%d{yyyy-MM-dd-HH}-%i.log\" > <PatternLayout pattern= \"%d %p [%c] - &lt;%m&gt;%n\" /> <Policies> <!-- Not using OnStartupTriggering or SizeBasedTriggering Policies <OnStartupTriggeringPolicy /> <SizeBasedTriggeringPolicy size=\"10 MB\"/> --> <TimeBasedTriggeringPolicy interval= \"1\" module= \"true\" /> </Policies> </RollingFile> <RollingFile name= \"auditlogfile\" fileName= \"${baseDir}/cas_audit.log\" append= \"true\" filePattern= \"${baseDir}/cas_audit-%d{yyyy-MM-dd-HH}.log\" > <PatternLayout pattern= \"%d %p [%c] - %m%n\" /> <Policies> <!-- Not using OnStartupTriggering or SizeBasedTriggering Policies <OnStartupTriggeringPolicy /> <SizeBasedTriggeringPolicy size=\"10 MB\"/> --> <TimeBasedTriggeringPolicy interval= \"1\" module= \"true\" /> </Policies> </RollingFile> These changes have been incorporated into our ansible playbook - as you'll see in the next section.","title":"Alter the log file rotation strategy (optional)"},{"location":"building-cas/create-gradle-overlay/","text":"Create a Gradle WAR overlay project Clone the CAS overlay template from Apereo's GitHub We're going to use git to clone the Apereo GitHub repository. In the command below - I'm specifically choosing the 6.2.x branch. The master branch of deployment is 6.3.x now - but could move forward at any time. I recommend being explicit with the branch you choose. git clone --branch 6.2 https://github.com/apereo/cas-overlay-template.git This will create a 'cas-overlay-template' in your home or build directory. Create a local branch After you\u2019re on the right branch (for our project, you should be on the master branch), create a new branch local to your project, which will be used to track all of your changes and keep them separate from any changes made to the template by the CAS developers. This will make it easier in the future to merge upstream changes from the CAS project team into your local template without having to redo all your changes. Choose a meaningful name for your branch, but not somthing likely to be duplicated by the CAS developers\u2014for example, newschool-casdev. Run the command git checkout -b yourschool-casdev and you should see notice that you've switched to that branch. Test build process Change to the newly created cas-overlay-template directory and run an initial build. This helps test things before we make any of our own modifications. [chauvetp@login6deva casbuild]$ cd cas-overlay-template/ [chauvetp@login6deva cas-overlay-template]$ ./gradlew clean build Starting a Gradle Daemon (subsequent builds will be faster) Deprecated Gradle features were used in this build, making it incompatible with Gradle 7.0. Use '--warning-mode all' to show the individual deprecation warnings. See https://docs.gradle.org/6.4/userguide/command_line_interface.html#sec:command_line_warnings BUILD SUCCESSFUL in 20s 5 actionable tasks: 5 executed When that's complete - you will have a cas.war file created within the build/libs subdirectory. As a quick test, you can copy that over to one of your CAS/Tomcat development hosts: scp build/libs/cas.war root@<One-of-Your-CAS-Hosts>:/opt/tomcat/latest/webapps/ Once you do that, if Tomcat is running and you wait a minute or two, you'll have CAS up and running - though not in a really useful way (no authentication backends, no external services, no ticket registry, no theme, etc.). It won't work to login - there's not even a configuration deployed - but it's just an initial test. References CAS 6.2.x deployment - WAR Overlays","title":"Create Gradle Overlay"},{"location":"building-cas/create-gradle-overlay/#create-a-gradle-war-overlay-project","text":"","title":"Create a Gradle WAR overlay project"},{"location":"building-cas/create-gradle-overlay/#clone-the-cas-overlay-template-from-apereos-github","text":"We're going to use git to clone the Apereo GitHub repository. In the command below - I'm specifically choosing the 6.2.x branch. The master branch of deployment is 6.3.x now - but could move forward at any time. I recommend being explicit with the branch you choose. git clone --branch 6.2 https://github.com/apereo/cas-overlay-template.git This will create a 'cas-overlay-template' in your home or build directory.","title":"Clone the CAS overlay template from Apereo's GitHub"},{"location":"building-cas/create-gradle-overlay/#create-a-local-branch","text":"After you\u2019re on the right branch (for our project, you should be on the master branch), create a new branch local to your project, which will be used to track all of your changes and keep them separate from any changes made to the template by the CAS developers. This will make it easier in the future to merge upstream changes from the CAS project team into your local template without having to redo all your changes. Choose a meaningful name for your branch, but not somthing likely to be duplicated by the CAS developers\u2014for example, newschool-casdev. Run the command git checkout -b yourschool-casdev and you should see notice that you've switched to that branch.","title":"Create a local branch"},{"location":"building-cas/create-gradle-overlay/#test-build-process","text":"Change to the newly created cas-overlay-template directory and run an initial build. This helps test things before we make any of our own modifications. [chauvetp@login6deva casbuild]$ cd cas-overlay-template/ [chauvetp@login6deva cas-overlay-template]$ ./gradlew clean build Starting a Gradle Daemon (subsequent builds will be faster) Deprecated Gradle features were used in this build, making it incompatible with Gradle 7.0. Use '--warning-mode all' to show the individual deprecation warnings. See https://docs.gradle.org/6.4/userguide/command_line_interface.html#sec:command_line_warnings BUILD SUCCESSFUL in 20s 5 actionable tasks: 5 executed When that's complete - you will have a cas.war file created within the build/libs subdirectory. As a quick test, you can copy that over to one of your CAS/Tomcat development hosts: scp build/libs/cas.war root@<One-of-Your-CAS-Hosts>:/opt/tomcat/latest/webapps/ Once you do that, if Tomcat is running and you wait a minute or two, you'll have CAS up and running - though not in a really useful way (no authentication backends, no external services, no ticket registry, no theme, etc.). It won't work to login - there's not even a configuration deployed - but it's just an initial test.","title":"Test build process"},{"location":"building-cas/create-gradle-overlay/#references","text":"CAS 6.2.x deployment - WAR Overlays","title":"References"},{"location":"building-cas/initial-cas-config/","text":"Initial CAS Configuration By default, CAS expects to find its configuration files in the operating system directory /etc/cas. Almost every aspect of CAS server configuration is controlled via settings stored in the cas.properties file located in the /etc/cas/config directory. We're going to start with a simple config - and enhance it as features are added. Tomcat will also expect the .war file to be placed within /opt/tomcat/latest/webapps (aka /var/lib/tomcat which is where that symlink points to). We'll be handling both the config and the cas.war file in this page. Tip The config files are for the CAS servers. If you're not using Ansible - you'll have to get them into /etc/cas/config/ on the indivudal CAS servers on your own. I'll be assuming Ansible going forward. If you're not using Ansible - you can still use this guide to create configuration files - you'll just have to fill in the variables manually instead of letting Ansible substitute them from vars files. Create Ansible CAS role Before we start - let's create a new role for cas6 - just like we did for apache-tomcat. Go to the roles directory and initialize a new role for cas6: [chauvetp@ansible ~]$ cd ansible/roles/ [chauvetp@ansible roles]$ ansible-galaxy init cas6 - Role cas was created successfully [chauvetp@ansible roles]$ ls cas6/ defaults files handlers meta README.md tasks templates tests vars Start creating configuration files Create a new file called 'dev-cas.properties.j2' (j2 indicates a Jinja2 template file). We're going to have variables in these files that we are NOT going to want to keep in the properties file - at least not in a git repository. I recommend one file for each of your environments (dev-cas.properties.j2, test-cas.properties.j2, prod-cas.properties.j2). All will eventually just be /etc/cas/config/cas.properties on their respective servers though. It's going to have some variables which we're going to fill in later. dev-cas.properties.j2 creation # Replace this with your public facing cas server name. If you're using a load balancer # This will be the virtual host within that load balancer, not the servers being the load balancer. cas.server.name=https://your-dev-server.domain.edu cas.server.prefix=${cas.server.name}/cas logging.config : file:/etc/cas/config/log4j2.xml # JSON Service Registry cas.serviceRegistry.json.location=file:/etc/cas/services cas.tgc.secure : true cas.tgc.crypto.signing.key : {{ dev_tgc_signing_key }} cas.tgc.crypto.encryption.key : {{ dev_tgc_encryption_key }} cas.webflow.crypto.signing.key : {{ dev_webflow_signing_key }} cas.webflow.crypto.encryption.key : {{ dev_webflow_encryption_key }} # Default handler - enable only for testing - leave blank (not commented out) to disable cas.authn.accept.users=YourTestUser::YourTestPassword Caution Within this file - the 'cas.authn.accept.users' value has been set. This is for testing only! It should be removed long before you are ready to go with production! Within the file - you'll notice a few areas with {{ variables_in_curly_brackets }}. These are variables. Many of them are sensitive values that you don't want unencrypted in a git repository (or not in a git repository at all). There's two ways to handle this: Using Ansible Vault (encrypted files - you're prompted for the encryption password when you run a playbook with the --ask-vault-pass option) Using .gitignore files to exclude one or more variable files from your git repo. You can combine these (i.e. using ansible vault and still using gitignore to not put those files into git.) Setting up an Ansible Vault file for sensitive variables Ansible gives the option of encrypting individual variables in a file, or encrypting the entire file. I tend to encrypt the entire file. At my organization there's only two people who need access to these encrypted files, so I don't mind sharing that password between two of us. For larger groups, there are options on encrypting files with multiple per-user keys (see the References section at the bottom for more tails). We're going to go with encrypted files with a single key here. From your CAS role directory: ansible-vault create vars/cas-vault.yml You'll be prompted to enter, and re-enter, a password/passphrase. Choose something good - and store that password somewhere like your password manager. It will then bring you into your system's default editor (if you're on Linux, vi by default). You can put placeholders for the variables we need there: dev_tgc_signing_key : dev_tgc_encryption_key : dev_webflow_signing_key : dev_webflow_encryption_key : Save the file for now. You can edit again via the following (you'll be prompted for your password): ansible-vault edit vars/cas-vault.yml Configure ticket granting cookie encryption The CAS server uses a ticket granting cookie in the browser to maintain login state during single sign-on sessions. A client can present this cookie to CAS in lieu of primary credentials and, provided it is valid, will be authenticated. The contents of the cookie should be encrypted to protect them, and when running in a multi-node environment, all of the nodes must use the same keys. These are defined in our cas.properties sample above as cas.tgc.crypto.signing.key and cas.tgc.crypto.encryption.key To generate the first key (cas.tgc.crypto.signing.key): Now visit the JSON Web Key Generator . Click on the oct tab. Set Key Size to 512, Key Use to Signature, and Algorithm to HS256. Click Generate . You'll see a Shared Key box. Find the value for \"k\" there, and copy it. Enter that as your dev_tgc_signing_key in your Ansible vault (or if not using Ansible vault - just put it directly as the value for cas.tgc.crypto.signing.key ) To generate the second key (cas.tgc.crypto.encryption.key): While still at the JSON Web Key Generator in the 'oct' tab: Set Key Size to 256, and keep the other settings, Key Use to Signature, and Algorithm to HS256, the same. Click Generate . You'll see a Shared Key box. Find the value for \"k\" there, and copy it. Enter that as your dev_tgc_encryption_key in your Ansible vault (or if not using Ansible vault - just put it directly as the value for cas.tgc.crypto.encryption.key ) Configure Spring Webflow encryption CAS uses Spring Webflow to manage the authentication sequence, and this also needs to be encrypted. Generate the first key (cas.webflow.crypto.signing.key): While still at the JSON Web Key Generator in the 'oct' tab: Set Key Size to 512, and keep the other settings, Key Use to Signature, and Algorithm to HS256, the same. Click Generate . You'll see a Shared Key box. Find the value for \"k\" there, and copy it. Enter that as your dev_webflow_signing_key in your Ansible vault (or if not using Ansible vault - just put it directly as the value for cas.webflow.crypto.signing.key ) Generate the second key (cas.webflow.crypto.encryption.key): This one is different. Unlike the ticket granting cookie encryption key above, the encryption key for Spring WebFlow is not a JSON Web Key. Rather, it\u2019s a randomly-generated string of 16 (by default) octets, Base64-encoded. An easy way to generate this key is to use openssl: [chauvetp@login6deva tmp]$ openssl rand -base64 16 NwriHAJ1Li2YnKxBbhUEVw== [chauvetp@login6deva tmp]$ Enter that as your dev_webflow_encryption_key in your Ansible vault (or if not using Ansible vault - just put it directly as the value for cas.webflow.crypto.encryption.key ) The online JSON Web Key Generator, which point to to generate keys, is provided by the Mitre Corporation and the MIT Kerberos and Internet Trust Consortium, and is simply a web-based interface to the json-web-key-generator project, also provided by Mitre/MIT. The project can be cloned from GitHub and built locally if you don\u2019t trust the online generator, or you can download and use a pre-built copy from the CAS project by running the command: curl -LO https://raw.githubusercontent.com/apereo/cas/master/etc/jwk-gen.jar Keys can then be generated using the command java -jar jwk-gen.jar -t oct -s [size] Save your vault file That's it for the vault file for now - save it. References Encrypting content with Ansible Vault","title":"Initial CAS config"},{"location":"building-cas/initial-cas-config/#initial-cas-configuration","text":"By default, CAS expects to find its configuration files in the operating system directory /etc/cas. Almost every aspect of CAS server configuration is controlled via settings stored in the cas.properties file located in the /etc/cas/config directory. We're going to start with a simple config - and enhance it as features are added. Tomcat will also expect the .war file to be placed within /opt/tomcat/latest/webapps (aka /var/lib/tomcat which is where that symlink points to). We'll be handling both the config and the cas.war file in this page. Tip The config files are for the CAS servers. If you're not using Ansible - you'll have to get them into /etc/cas/config/ on the indivudal CAS servers on your own. I'll be assuming Ansible going forward. If you're not using Ansible - you can still use this guide to create configuration files - you'll just have to fill in the variables manually instead of letting Ansible substitute them from vars files.","title":"Initial CAS Configuration"},{"location":"building-cas/initial-cas-config/#create-ansible-cas-role","text":"Before we start - let's create a new role for cas6 - just like we did for apache-tomcat. Go to the roles directory and initialize a new role for cas6: [chauvetp@ansible ~]$ cd ansible/roles/ [chauvetp@ansible roles]$ ansible-galaxy init cas6 - Role cas was created successfully [chauvetp@ansible roles]$ ls cas6/ defaults files handlers meta README.md tasks templates tests vars","title":"Create Ansible CAS role"},{"location":"building-cas/initial-cas-config/#start-creating-configuration-files","text":"Create a new file called 'dev-cas.properties.j2' (j2 indicates a Jinja2 template file). We're going to have variables in these files that we are NOT going to want to keep in the properties file - at least not in a git repository. I recommend one file for each of your environments (dev-cas.properties.j2, test-cas.properties.j2, prod-cas.properties.j2). All will eventually just be /etc/cas/config/cas.properties on their respective servers though. It's going to have some variables which we're going to fill in later.","title":"Start creating configuration files"},{"location":"building-cas/initial-cas-config/#dev-caspropertiesj2-creation","text":"# Replace this with your public facing cas server name. If you're using a load balancer # This will be the virtual host within that load balancer, not the servers being the load balancer. cas.server.name=https://your-dev-server.domain.edu cas.server.prefix=${cas.server.name}/cas logging.config : file:/etc/cas/config/log4j2.xml # JSON Service Registry cas.serviceRegistry.json.location=file:/etc/cas/services cas.tgc.secure : true cas.tgc.crypto.signing.key : {{ dev_tgc_signing_key }} cas.tgc.crypto.encryption.key : {{ dev_tgc_encryption_key }} cas.webflow.crypto.signing.key : {{ dev_webflow_signing_key }} cas.webflow.crypto.encryption.key : {{ dev_webflow_encryption_key }} # Default handler - enable only for testing - leave blank (not commented out) to disable cas.authn.accept.users=YourTestUser::YourTestPassword Caution Within this file - the 'cas.authn.accept.users' value has been set. This is for testing only! It should be removed long before you are ready to go with production! Within the file - you'll notice a few areas with {{ variables_in_curly_brackets }}. These are variables. Many of them are sensitive values that you don't want unencrypted in a git repository (or not in a git repository at all). There's two ways to handle this: Using Ansible Vault (encrypted files - you're prompted for the encryption password when you run a playbook with the --ask-vault-pass option) Using .gitignore files to exclude one or more variable files from your git repo. You can combine these (i.e. using ansible vault and still using gitignore to not put those files into git.)","title":"dev-cas.properties.j2 creation"},{"location":"building-cas/initial-cas-config/#setting-up-an-ansible-vault-file-for-sensitive-variables","text":"Ansible gives the option of encrypting individual variables in a file, or encrypting the entire file. I tend to encrypt the entire file. At my organization there's only two people who need access to these encrypted files, so I don't mind sharing that password between two of us. For larger groups, there are options on encrypting files with multiple per-user keys (see the References section at the bottom for more tails). We're going to go with encrypted files with a single key here. From your CAS role directory: ansible-vault create vars/cas-vault.yml You'll be prompted to enter, and re-enter, a password/passphrase. Choose something good - and store that password somewhere like your password manager. It will then bring you into your system's default editor (if you're on Linux, vi by default). You can put placeholders for the variables we need there: dev_tgc_signing_key : dev_tgc_encryption_key : dev_webflow_signing_key : dev_webflow_encryption_key : Save the file for now. You can edit again via the following (you'll be prompted for your password): ansible-vault edit vars/cas-vault.yml","title":"Setting up an Ansible Vault file for sensitive variables"},{"location":"building-cas/initial-cas-config/#configure-ticket-granting-cookie-encryption","text":"The CAS server uses a ticket granting cookie in the browser to maintain login state during single sign-on sessions. A client can present this cookie to CAS in lieu of primary credentials and, provided it is valid, will be authenticated. The contents of the cookie should be encrypted to protect them, and when running in a multi-node environment, all of the nodes must use the same keys. These are defined in our cas.properties sample above as cas.tgc.crypto.signing.key and cas.tgc.crypto.encryption.key","title":"Configure ticket granting cookie encryption"},{"location":"building-cas/initial-cas-config/#to-generate-the-first-key-castgccryptosigningkey","text":"Now visit the JSON Web Key Generator . Click on the oct tab. Set Key Size to 512, Key Use to Signature, and Algorithm to HS256. Click Generate . You'll see a Shared Key box. Find the value for \"k\" there, and copy it. Enter that as your dev_tgc_signing_key in your Ansible vault (or if not using Ansible vault - just put it directly as the value for cas.tgc.crypto.signing.key )","title":"To generate the first key (cas.tgc.crypto.signing.key):"},{"location":"building-cas/initial-cas-config/#to-generate-the-second-key-castgccryptoencryptionkey","text":"While still at the JSON Web Key Generator in the 'oct' tab: Set Key Size to 256, and keep the other settings, Key Use to Signature, and Algorithm to HS256, the same. Click Generate . You'll see a Shared Key box. Find the value for \"k\" there, and copy it. Enter that as your dev_tgc_encryption_key in your Ansible vault (or if not using Ansible vault - just put it directly as the value for cas.tgc.crypto.encryption.key )","title":"To generate the second key (cas.tgc.crypto.encryption.key):"},{"location":"building-cas/initial-cas-config/#configure-spring-webflow-encryption","text":"CAS uses Spring Webflow to manage the authentication sequence, and this also needs to be encrypted.","title":"Configure Spring Webflow encryption"},{"location":"building-cas/initial-cas-config/#generate-the-first-key-caswebflowcryptosigningkey","text":"While still at the JSON Web Key Generator in the 'oct' tab: Set Key Size to 512, and keep the other settings, Key Use to Signature, and Algorithm to HS256, the same. Click Generate . You'll see a Shared Key box. Find the value for \"k\" there, and copy it. Enter that as your dev_webflow_signing_key in your Ansible vault (or if not using Ansible vault - just put it directly as the value for cas.webflow.crypto.signing.key )","title":"Generate the first key (cas.webflow.crypto.signing.key):"},{"location":"building-cas/initial-cas-config/#generate-the-second-key-caswebflowcryptoencryptionkey","text":"This one is different. Unlike the ticket granting cookie encryption key above, the encryption key for Spring WebFlow is not a JSON Web Key. Rather, it\u2019s a randomly-generated string of 16 (by default) octets, Base64-encoded. An easy way to generate this key is to use openssl: [chauvetp@login6deva tmp]$ openssl rand -base64 16 NwriHAJ1Li2YnKxBbhUEVw== [chauvetp@login6deva tmp]$ Enter that as your dev_webflow_encryption_key in your Ansible vault (or if not using Ansible vault - just put it directly as the value for cas.webflow.crypto.encryption.key ) The online JSON Web Key Generator, which point to to generate keys, is provided by the Mitre Corporation and the MIT Kerberos and Internet Trust Consortium, and is simply a web-based interface to the json-web-key-generator project, also provided by Mitre/MIT. The project can be cloned from GitHub and built locally if you don\u2019t trust the online generator, or you can download and use a pre-built copy from the CAS project by running the command: curl -LO https://raw.githubusercontent.com/apereo/cas/master/etc/jwk-gen.jar Keys can then be generated using the command java -jar jwk-gen.jar -t oct -s [size]","title":"Generate the second key (cas.webflow.crypto.encryption.key):"},{"location":"building-cas/initial-cas-config/#save-your-vault-file","text":"That's it for the vault file for now - save it.","title":"Save your vault file"},{"location":"building-cas/initial-cas-config/#references","text":"Encrypting content with Ansible Vault","title":"References"},{"location":"building-cas/overview/","text":"Building the CAS server If you've followed the other steps, then you'll already have one or more Tomcat servers ready to go. Overview We will be building from source, using the Gradle WAR overlay method. As per Misagh Moayyed's \"Getting Started\" document: CAS 6.0.x Deployment - WAR Overlays - (https://apereo.github.io/2019/11/03/cas62-gettingstarted-overlay/) Overlays are a strategy to combat repetitive code and/or resources. Rather than downloading the CAS codebase and building it from source, overlays allow you to download a pre-built vanilla CAS web application server provided by the project itself, override/insert specific behavior into it and then merge it all back together to produce the final (web application) artifact. If you've used CAS 5.x, you may have used the Maven WAR overlay template instead, but as of CAS 6 this was deprecated and the Gradle WAR overlay method is used instead. Before you get started These steps are done on the build server - not the CAS servers. We'll get to pushing the build out to the CAS servers later. On your build server, you'll want to have installed: java-11-openjdk java-11-openjdk-devel git curl-devel I may have forgotten some prerequisites since I'm doing this on an existing server. I will be going back to replicate this in the future on a clean server and will catch anything I missed there. References CAS 6.2.x deployment - WAR Overlays","title":"Overview"},{"location":"building-cas/overview/#building-the-cas-server","text":"If you've followed the other steps, then you'll already have one or more Tomcat servers ready to go.","title":"Building the CAS server"},{"location":"building-cas/overview/#overview","text":"We will be building from source, using the Gradle WAR overlay method. As per Misagh Moayyed's \"Getting Started\" document: CAS 6.0.x Deployment - WAR Overlays - (https://apereo.github.io/2019/11/03/cas62-gettingstarted-overlay/) Overlays are a strategy to combat repetitive code and/or resources. Rather than downloading the CAS codebase and building it from source, overlays allow you to download a pre-built vanilla CAS web application server provided by the project itself, override/insert specific behavior into it and then merge it all back together to produce the final (web application) artifact. If you've used CAS 5.x, you may have used the Maven WAR overlay template instead, but as of CAS 6 this was deprecated and the Gradle WAR overlay method is used instead.","title":"Overview"},{"location":"building-cas/overview/#before-you-get-started","text":"These steps are done on the build server - not the CAS servers. We'll get to pushing the build out to the CAS servers later. On your build server, you'll want to have installed: java-11-openjdk java-11-openjdk-devel git curl-devel I may have forgotten some prerequisites since I'm doing this on an existing server. I will be going back to replicate this in the future on a clean server and will catch anything I missed there.","title":"Before you get started"},{"location":"building-cas/overview/#references","text":"CAS 6.2.x deployment - WAR Overlays","title":"References"},{"location":"cas-client/httpd-cas-client/","text":"Configuring HTTPD to use CAS","title":"Configure httpd to use CAS"},{"location":"cas-client/httpd-cas-client/#configuring-httpd-to-use-cas","text":"","title":"Configuring HTTPD to use CAS"},{"location":"cas-client/overview/","text":"CAS Client Overview","title":"CAS client overview"},{"location":"cas-client/overview/#cas-client-overview","text":"","title":"CAS Client Overview"},{"location":"cas-client/test-cas-client/","text":"Testing the CAS Client","title":"Test the application"},{"location":"cas-client/test-cas-client/#testing-the-cas-client","text":"","title":"Testing the CAS Client"},{"location":"service-config/overview/","text":"Service configuration Services within CAS are where you define a policy per-service. For example you may want to: Release certain attributes to one service (name, email, eduPerson values, etc.) Require a user to be in one or more groups in Active Directory, or to have a certain eduPersonPrimaryAffiliation before they can access a particular service. Use a multi-factor authentication policy on a given service. Each service can be different. You can combine functions in the JSON service to accomplish various tasks. We'll get to more of those later. There are several ways to handle service registries, including the CAS Services Management Webapp . Because I deploy and maintain CAS via Ansible, I prefer to define my services via JSON files within /etc/cas/services and deploy via Ansible instead of using a web application. This helps preserve history (via git) of service changes. It's all up to you though. This documentation will assume you're using JSON files here. To use the JSON service registry, you're going to want to edit your build.gradle file within your cas-overlay-template directory. Editing build.gradle Look for this section, around line 72 in the build.gradle file dependencies { // Other CAS dependencies/modules may be listed here... // implementation \"org.apereo.cas:cas-server-support-json-service-registry:${casServerVersion}\" } Add the dependency compile \"org.apereo.cas:cas-server-support-json-service-registry:${project.'cas.version'}\" as follows there: dependencies { // Other CAS dependencies/modules may be listed here... // implementation \"org.apereo.cas:cas-server-support-json-service-registry:${casServerVersion}\" compile \"org.apereo.cas:cas-server-support-json-service-registry:${project.'cas.version'}\" } Update your cas.properties Go to your dev-cas.properties.j2 file in your ansible templates directory (or update the file cas.properties file directly if you're not using ansible) and make sure the following entry exists (it should be from our initial CAS config): # JSON Service Registry cas.serviceRegistry.json.location=file:/etc/cas/services Create a test service definition file Within the templates directory (in your CAS Ansible role), create a subdirectory called dev-services . For simplicity (and to avoid worrying about the details of the service registry for the moment), create a \u201cwildcard\u201d service definition that will allow any HTTPS based service to make use of the CAS server. Create a file in the dev-services directory with the following contents: { /* * Wildcard service de f i n i t io n t ha t applies t o a n y h tt ps or imaps url. * Do n o t use t his de f i n i t io n i n a produc t io n e n viro n me nt . */ \"@class\" : \"org.apereo.cas.services.RegexRegisteredService\" , \"serviceId\" : \"^https://.*\" , \"name\" : \"HTTPS wildcard\" , \"id\" : 1614029117 , \"evaluationOrder\" : 99999 } The CAS documentation recommends the following naming convention for JSON service definition files: JSON filename = serviceName + \"-\" + serviceNumericId + \".json\" Therefore, the filename for the wildcard service definition above should be: HTTPSWildcard-1614029117.json Wait - where'd those crazy number come from? Techincally the id number just needs to be something unique. One way to do this is with the date +%s command in Linux, which will give you the date/time as the number of seconds in the unix epoch. The 'evaluationOrder' is set really high here since it will process anything here - but you want a wildcard like this to run last. The fields we've used so far are defined as: Field Description 'serviceId' A regular expression describing the URL(s) where a service or services are located. Care should be taken to avoid patterns that match more than just the desired URL(s), as this can create security vulnerabilities. 'name' A name for the service. Note that because the service definition filename is created based on this name (see above), the value of this field should never contain characters that are not allowed in filenames . 'id' Unique numeric identifier for the service definition. An easy way to ensure that these identifiers are unique is to use the date and time the service definition was created. This can be represented as YYYYMMDDhhmmss or, for a more \u201canonymous\u201d representation, as a timestamp (number of seconds since the epoch), which can be obtained with the command date +%s . 'evaluationOrder' A value that determines the relative evaluation order of registered services (lower values come before higher values). This is especially important when more than one serviceId expression can match the same service; evalutionOrder deterines which expression is evaluated first. Getting your services from Ansible to CAS As listed earlier, I recommend creating a separate service directory for each tier (prod, test, dev) within your templates directory. I've created one called dev-services (within the templates directory of your Ansible CAS role). Place your newly created service file from above into this directory. Update your tasks Within the tasks directory, create a new file called service-config.yml (or something similar). Reference that file within your main.yml - which should now look like: --- # tasks file for cas6 - include_tasks : base-cas-config.yml - include_tasks : service-config.yml Your service-config.yml should look like the following: --- - name : Ensure service files are populated from templates template : src : '{{ item.src }}' dest : '/etc/cas/services/{{ item.path }}' owner : root group : tomcat with_filetree : '../templates/dev-services' when : item.state == 'file' and 'login6dev' in inventory_hostname This is a way of getting a whole directory copied over instead of a single file. Otherwise you'd have to define an ansible template for each service you have - which could get pretty large (though is okay if you prefer it!). Rebuild and redeploy CAS To rebuild CAS with this dependency built in, you'll want to go to the cas-overlay-template directory and run the following and wait a moment for it to be complete: ./gradlew clean build Your cas.war file will have been updated. You can either copy it to your CAS server directly (but where's the fun in that!) or update it in your files directory in your CAS Ansible role. If you update it in your Ansible role, you can just rerun the playbook. [ chauvetp@ansible templates ] $ ansible-playbook ~/ansible/site.yml --ask-vault-pass --limit <your_CAS_server> Vault password : It won't have to rebuild or reinstall Tomcat, since it's idempotent. It will only alter what is different from our templates/files/playbook. In fact, when you run it, it will say 'ok' or 'skipped' for every task except the following (assuming you didn't change anything else), where it will say changed : TASK [cas6 : Configure cas.properties file (dev)] TASK [cas6 : Copy CAS war file] RUNNING HANDLER [cas6 : restart tomcat] References CAS Services Management","title":"Service Overview"},{"location":"service-config/overview/#service-configuration","text":"Services within CAS are where you define a policy per-service. For example you may want to: Release certain attributes to one service (name, email, eduPerson values, etc.) Require a user to be in one or more groups in Active Directory, or to have a certain eduPersonPrimaryAffiliation before they can access a particular service. Use a multi-factor authentication policy on a given service. Each service can be different. You can combine functions in the JSON service to accomplish various tasks. We'll get to more of those later. There are several ways to handle service registries, including the CAS Services Management Webapp . Because I deploy and maintain CAS via Ansible, I prefer to define my services via JSON files within /etc/cas/services and deploy via Ansible instead of using a web application. This helps preserve history (via git) of service changes. It's all up to you though. This documentation will assume you're using JSON files here. To use the JSON service registry, you're going to want to edit your build.gradle file within your cas-overlay-template directory.","title":"Service configuration"},{"location":"service-config/overview/#editing-buildgradle","text":"Look for this section, around line 72 in the build.gradle file dependencies { // Other CAS dependencies/modules may be listed here... // implementation \"org.apereo.cas:cas-server-support-json-service-registry:${casServerVersion}\" } Add the dependency compile \"org.apereo.cas:cas-server-support-json-service-registry:${project.'cas.version'}\" as follows there: dependencies { // Other CAS dependencies/modules may be listed here... // implementation \"org.apereo.cas:cas-server-support-json-service-registry:${casServerVersion}\" compile \"org.apereo.cas:cas-server-support-json-service-registry:${project.'cas.version'}\" }","title":"Editing build.gradle"},{"location":"service-config/overview/#update-your-casproperties","text":"Go to your dev-cas.properties.j2 file in your ansible templates directory (or update the file cas.properties file directly if you're not using ansible) and make sure the following entry exists (it should be from our initial CAS config): # JSON Service Registry cas.serviceRegistry.json.location=file:/etc/cas/services","title":"Update your cas.properties"},{"location":"service-config/overview/#create-a-test-service-definition-file","text":"Within the templates directory (in your CAS Ansible role), create a subdirectory called dev-services . For simplicity (and to avoid worrying about the details of the service registry for the moment), create a \u201cwildcard\u201d service definition that will allow any HTTPS based service to make use of the CAS server. Create a file in the dev-services directory with the following contents: { /* * Wildcard service de f i n i t io n t ha t applies t o a n y h tt ps or imaps url. * Do n o t use t his de f i n i t io n i n a produc t io n e n viro n me nt . */ \"@class\" : \"org.apereo.cas.services.RegexRegisteredService\" , \"serviceId\" : \"^https://.*\" , \"name\" : \"HTTPS wildcard\" , \"id\" : 1614029117 , \"evaluationOrder\" : 99999 } The CAS documentation recommends the following naming convention for JSON service definition files: JSON filename = serviceName + \"-\" + serviceNumericId + \".json\" Therefore, the filename for the wildcard service definition above should be: HTTPSWildcard-1614029117.json","title":"Create a test service definition file"},{"location":"service-config/overview/#wait-whered-those-crazy-number-come-from","text":"Techincally the id number just needs to be something unique. One way to do this is with the date +%s command in Linux, which will give you the date/time as the number of seconds in the unix epoch. The 'evaluationOrder' is set really high here since it will process anything here - but you want a wildcard like this to run last. The fields we've used so far are defined as: Field Description 'serviceId' A regular expression describing the URL(s) where a service or services are located. Care should be taken to avoid patterns that match more than just the desired URL(s), as this can create security vulnerabilities. 'name' A name for the service. Note that because the service definition filename is created based on this name (see above), the value of this field should never contain characters that are not allowed in filenames . 'id' Unique numeric identifier for the service definition. An easy way to ensure that these identifiers are unique is to use the date and time the service definition was created. This can be represented as YYYYMMDDhhmmss or, for a more \u201canonymous\u201d representation, as a timestamp (number of seconds since the epoch), which can be obtained with the command date +%s . 'evaluationOrder' A value that determines the relative evaluation order of registered services (lower values come before higher values). This is especially important when more than one serviceId expression can match the same service; evalutionOrder deterines which expression is evaluated first.","title":"Wait - where'd those crazy number come from?"},{"location":"service-config/overview/#getting-your-services-from-ansible-to-cas","text":"As listed earlier, I recommend creating a separate service directory for each tier (prod, test, dev) within your templates directory. I've created one called dev-services (within the templates directory of your Ansible CAS role). Place your newly created service file from above into this directory.","title":"Getting your services from Ansible to CAS"},{"location":"service-config/overview/#update-your-tasks","text":"Within the tasks directory, create a new file called service-config.yml (or something similar). Reference that file within your main.yml - which should now look like: --- # tasks file for cas6 - include_tasks : base-cas-config.yml - include_tasks : service-config.yml Your service-config.yml should look like the following: --- - name : Ensure service files are populated from templates template : src : '{{ item.src }}' dest : '/etc/cas/services/{{ item.path }}' owner : root group : tomcat with_filetree : '../templates/dev-services' when : item.state == 'file' and 'login6dev' in inventory_hostname This is a way of getting a whole directory copied over instead of a single file. Otherwise you'd have to define an ansible template for each service you have - which could get pretty large (though is okay if you prefer it!).","title":"Update your tasks"},{"location":"service-config/overview/#rebuild-and-redeploy-cas","text":"To rebuild CAS with this dependency built in, you'll want to go to the cas-overlay-template directory and run the following and wait a moment for it to be complete: ./gradlew clean build Your cas.war file will have been updated. You can either copy it to your CAS server directly (but where's the fun in that!) or update it in your files directory in your CAS Ansible role. If you update it in your Ansible role, you can just rerun the playbook. [ chauvetp@ansible templates ] $ ansible-playbook ~/ansible/site.yml --ask-vault-pass --limit <your_CAS_server> Vault password : It won't have to rebuild or reinstall Tomcat, since it's idempotent. It will only alter what is different from our templates/files/playbook. In fact, when you run it, it will say 'ok' or 'skipped' for every task except the following (assuming you didn't change anything else), where it will say changed : TASK [cas6 : Configure cas.properties file (dev)] TASK [cas6 : Copy CAS war file] RUNNING HANDLER [cas6 : restart tomcat]","title":"Rebuild and redeploy CAS"},{"location":"service-config/overview/#references","text":"CAS Services Management","title":"References"},{"location":"setting-up-the-environment/build-environment/","text":"Build Environment Summary My build server is our existing Ansible host. I've set that system up several years ago - so I may have missed a step or two here. I'll try to recreate this from scratch later though. Install and Configure git We keep our code for git in an internal git repository running Bitbucket Server . We make enough changes to CAS and Ansible that I would be insane to not use git or some other form of version control for development. Whether you do or not is up to you - but the instructions here assume you are using some sort of git repository. If you aren't familiar with Git - I recommend Pro Git, available online for free via (https://git-scm.com/book/en/v2). Install and basic configuration of Git This should be done as the user you are going to be running Ansible and building CAS from - NOT root. yum install git git config --global user.name \"Your Name Here\" git config --global user.email \"Your Email Here\" Install Ansible We are using the version of Ansible that comes with RedHat. This was just installed via: yum install ansible Set up SSH public key authentication If you're deploying via Ansible, or just using SCP/SFTP to copy files over, you'll want to set up public key authentication. From the master server: When done - view the contents of the public key (by default in /home/username/.ssh/id_rsa.pub) - you'll need it in the next step. From the CAS servers: Edit the file /root/.ssh/authorized_keys Place the contents of the public key in that file. Test Login to your build server as the user you will be building CAS and running ansible from (I'll use 'builduser' as an example here). See if you can login to one of your newly built CAS servers without entering a password. ssh root@cas6dev1 If successful - you'll be logged in without a username or password. If this is the first time you are connecting via SSH from the build host to the CAS host, you'll be warned that the authenticity of the host cannot be established, and you'll be prompted to enter yes to continue connecting.","title":"Build Environment"},{"location":"setting-up-the-environment/build-environment/#build-environment","text":"Summary My build server is our existing Ansible host. I've set that system up several years ago - so I may have missed a step or two here. I'll try to recreate this from scratch later though.","title":"Build Environment"},{"location":"setting-up-the-environment/build-environment/#install-and-configure-git","text":"We keep our code for git in an internal git repository running Bitbucket Server . We make enough changes to CAS and Ansible that I would be insane to not use git or some other form of version control for development. Whether you do or not is up to you - but the instructions here assume you are using some sort of git repository. If you aren't familiar with Git - I recommend Pro Git, available online for free via (https://git-scm.com/book/en/v2).","title":"Install and Configure git"},{"location":"setting-up-the-environment/build-environment/#install-and-basic-configuration-of-git","text":"This should be done as the user you are going to be running Ansible and building CAS from - NOT root. yum install git git config --global user.name \"Your Name Here\" git config --global user.email \"Your Email Here\"","title":"Install and basic configuration of Git"},{"location":"setting-up-the-environment/build-environment/#install-ansible","text":"We are using the version of Ansible that comes with RedHat. This was just installed via: yum install ansible","title":"Install Ansible"},{"location":"setting-up-the-environment/build-environment/#set-up-ssh-public-key-authentication","text":"If you're deploying via Ansible, or just using SCP/SFTP to copy files over, you'll want to set up public key authentication.","title":"Set up SSH public key authentication"},{"location":"setting-up-the-environment/build-environment/#from-the-master-server","text":"When done - view the contents of the public key (by default in /home/username/.ssh/id_rsa.pub) - you'll need it in the next step.","title":"From the master server:"},{"location":"setting-up-the-environment/build-environment/#from-the-cas-servers","text":"Edit the file /root/.ssh/authorized_keys Place the contents of the public key in that file.","title":"From the CAS servers:"},{"location":"setting-up-the-environment/build-environment/#test","text":"Login to your build server as the user you will be building CAS and running ansible from (I'll use 'builduser' as an example here). See if you can login to one of your newly built CAS servers without entering a password. ssh root@cas6dev1 If successful - you'll be logged in without a username or password. If this is the first time you are connecting via SSH from the build host to the CAS host, you'll be warned that the authenticity of the host cannot be established, and you'll be prompted to enter yes to continue connecting.","title":"Test"},{"location":"setting-up-the-environment/overview/","text":"System/Architecture Overview Components Summary Before beginning the CAS build and configuration process, you will want to plan out your server environment. Below is how we have things setup - but by no means is the only way. We at New Paltz have a three-tiered CAS consisting of: Development: This is used exclusively for deploying/configuring/testing CAS itself - no end users use this. Test: This is used for test applications/environments which still need authentication such as Banner Test. Since there are a number of IT staff and end users testing these applications, changes here can still impact users though far less than production. Production: The actual public facing CAS. Within each tier, you will want one or more servers. We do something similar to: Development CAS6Dev1 CAS6Dev2 logindev: Public-facing load balanced virtual host Test CAS6Test1 CAS6Test2 logintest: Public-facing load balanced virtual host Production CAS6Prod1 CAS6Prod2 login: Public-facing load balanced virtual host Each of these servers for us are RHEL 8 hosts, with: 2 virtual CPUs for production, 1 for test/dev 4 GB memory (though can probably get away with less) 25 GB disk For the official requirements, see the Apereo CAS Installation Requirements document. I will document both how to set these up manually, as well as my Ansible playbooks which handle deployment (of Apache, Apache Tomcat, and CAS itself).","title":"Overview"},{"location":"setting-up-the-environment/overview/#systemarchitecture-overview","text":"","title":"System/Architecture Overview"},{"location":"setting-up-the-environment/overview/#components","text":"Summary Before beginning the CAS build and configuration process, you will want to plan out your server environment. Below is how we have things setup - but by no means is the only way. We at New Paltz have a three-tiered CAS consisting of: Development: This is used exclusively for deploying/configuring/testing CAS itself - no end users use this. Test: This is used for test applications/environments which still need authentication such as Banner Test. Since there are a number of IT staff and end users testing these applications, changes here can still impact users though far less than production. Production: The actual public facing CAS. Within each tier, you will want one or more servers. We do something similar to: Development CAS6Dev1 CAS6Dev2 logindev: Public-facing load balanced virtual host Test CAS6Test1 CAS6Test2 logintest: Public-facing load balanced virtual host Production CAS6Prod1 CAS6Prod2 login: Public-facing load balanced virtual host Each of these servers for us are RHEL 8 hosts, with: 2 virtual CPUs for production, 1 for test/dev 4 GB memory (though can probably get away with less) 25 GB disk For the official requirements, see the Apereo CAS Installation Requirements document. I will document both how to set these up manually, as well as my Ansible playbooks which handle deployment (of Apache, Apache Tomcat, and CAS itself).","title":"Components"},{"location":"setting-up-the-environment/time-synchronization/","text":"Configure Time Synchronization For an active-active environment with at least two servers in each pair, it's essential that the system clocks match. Out-of-date clocks between the CAS servers could cause issues with ticket processing, and will make correllating the logs (when doing an investigation - whether it be troubleshooting or incident response) problematic. The best way to handle this is to ensure the Network Time Protocol (NTP) is in use. Determine if NTP is already in use. RHEL 8 offers chrony for NTP time synchronization. Older versions of RHEL either had ntpd available, or in use by default, but chrony is what RHEL uses by default now. If you're looking for more about the differences between chronyd and ntpd, see Comparison of NTP implementations . That being said, I've never had issues using whatever the OS default is. # Check chrony status systemctl status chronyd # If you are on an older version of RHEL or have otherwise installed NTPD - you can check it as follows: systemctl status ntpd If chrony is running, you'll see something like: If the one you check is not installed - you'll see a message like: Unit chronyd.service could not be found Install chrony (if needed) If you don't have chrony installed, you can install it via dnf install chronyd Configure chrony Chrony by default is configured via /etc/chrony.conf. The default file from RedHat is below, though most options are commented out by default. The only change I've ever made to it is including our on-prem NTP servers. If you want to do that, remove the ntp.org servers from the list and replace or supplement them with your own server(s). # Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). server 0.rhel.pool.ntp.org iburst server 1.rhel.pool.ntp.org iburst server 2.rhel.pool.ntp.org iburst server 3.rhel.pool.ntp.org iburst # Record the rate at which the system clock gains/losses time. driftfile /var/lib/chrony/drift # Allow the system clock to be stepped in the first three updates # if its offset is larger than 1 second. makestep 1.0 3 # Enable kernel synchronization of the real-time clock (RTC). rtcsync # Enable hardware timestamping on all interfaces that support it. #hwtimestamp * # Increase the minimum number of selectable sources required to adjust # the system clock. #minsources 2 # Allow NTP client access from local network. #allow 192.168.0.0/16 # Serve time even if not synchronized to a time source. #local stratum 10 # Specify file containing keys for NTP authentication. #keyfile /etc/chrony.keys # Specify directory for log files. logdir /var/log/chrony # Select which information is logged. #log measurements statistics tracking Enable and start chronyd # Set chronyd to start on boot systemctl enable chronyd # Start chronyd now systemctl start chronyd You can check some information about time sync on the system with the command: chronyc tracking and chronyc sources .","title":"Configure time synchronization"},{"location":"setting-up-the-environment/time-synchronization/#configure-time-synchronization","text":"For an active-active environment with at least two servers in each pair, it's essential that the system clocks match. Out-of-date clocks between the CAS servers could cause issues with ticket processing, and will make correllating the logs (when doing an investigation - whether it be troubleshooting or incident response) problematic. The best way to handle this is to ensure the Network Time Protocol (NTP) is in use.","title":"Configure Time Synchronization"},{"location":"setting-up-the-environment/time-synchronization/#determine-if-ntp-is-already-in-use","text":"RHEL 8 offers chrony for NTP time synchronization. Older versions of RHEL either had ntpd available, or in use by default, but chrony is what RHEL uses by default now. If you're looking for more about the differences between chronyd and ntpd, see Comparison of NTP implementations . That being said, I've never had issues using whatever the OS default is. # Check chrony status systemctl status chronyd # If you are on an older version of RHEL or have otherwise installed NTPD - you can check it as follows: systemctl status ntpd If chrony is running, you'll see something like: If the one you check is not installed - you'll see a message like: Unit chronyd.service could not be found","title":"Determine if NTP is already in use."},{"location":"setting-up-the-environment/time-synchronization/#install-chrony-if-needed","text":"If you don't have chrony installed, you can install it via dnf install chronyd","title":"Install chrony (if needed)"},{"location":"setting-up-the-environment/time-synchronization/#configure-chrony","text":"Chrony by default is configured via /etc/chrony.conf. The default file from RedHat is below, though most options are commented out by default. The only change I've ever made to it is including our on-prem NTP servers. If you want to do that, remove the ntp.org servers from the list and replace or supplement them with your own server(s). # Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). server 0.rhel.pool.ntp.org iburst server 1.rhel.pool.ntp.org iburst server 2.rhel.pool.ntp.org iburst server 3.rhel.pool.ntp.org iburst # Record the rate at which the system clock gains/losses time. driftfile /var/lib/chrony/drift # Allow the system clock to be stepped in the first three updates # if its offset is larger than 1 second. makestep 1.0 3 # Enable kernel synchronization of the real-time clock (RTC). rtcsync # Enable hardware timestamping on all interfaces that support it. #hwtimestamp * # Increase the minimum number of selectable sources required to adjust # the system clock. #minsources 2 # Allow NTP client access from local network. #allow 192.168.0.0/16 # Serve time even if not synchronized to a time source. #local stratum 10 # Specify file containing keys for NTP authentication. #keyfile /etc/chrony.keys # Specify directory for log files. logdir /var/log/chrony # Select which information is logged. #log measurements statistics tracking","title":"Configure chrony"},{"location":"setting-up-the-environment/time-synchronization/#enable-and-start-chronyd","text":"# Set chronyd to start on boot systemctl enable chronyd # Start chronyd now systemctl start chronyd You can check some information about time sync on the system with the command: chronyc tracking and chronyc sources .","title":"Enable and start chronyd"},{"location":"setting-up-the-environment/httpd/ajp-proxy/","text":"Setting up Apache HTTPD in front of Tomcat I prefer, for a number of reasons, to put Apache httpd in front of Tomcat (even though there's a load balancer in front of the CAS servers as well. I'm sure there's ways around this, but I like the flexibility that it provides me, and I also have had other applications on the CAS servers that are not running on Tomcat and still need to be accessible (including SimpleSAML for applications we haven't moved to Azure or directly into CAS yet). For now, I'm not going to go over full configuration of Apache httpd here. I will eventually (hopefully by end of March 2021) put up my steps and Ansible playbooks to get httpd running, but for now - I'll mention the Tomcat/httpd specific portion. We use AJP for communication between Apache httpd and Apache Tomcat. Configuring this is in two steps, one on the httpd server and one on Tomcat. On the httpd server Create a configuration file in /etc/httpd/conf.d. We call ours 'cas-ajp.conf' but it doesn't matter as long as it ends in .conf. The contents of which are below: ProxyRequests Off <Proxy *> Order allow,deny Allow from all </Proxy> ProxyPass /cas ajp://localhost:8009/cas ProxyPassReverse /cas ajp://localhost:8009/cas On the Tomcat server Edit /etc/tomcat/server.xml and define an AJP port: <!-- Define an AJP 1.3 Connector on port 8009 See https://tomcat.apache.org/tomcat-9.0-doc/config/ajp.html for more on the 'secretRequired' and 'secret' options. Since I'm only exposing this to localhost via host firewall --> <Connector port= \"8009\" protocol= \"AJP/1.3\" redirectPort= \"8443\" secretRequired= \"false\" /> The Tomcat portion is already in the Tomcat server.xml that is linked to from the Tomcat/Ansible section.","title":"Use AJP to communicate with Tomcat"},{"location":"setting-up-the-environment/httpd/ajp-proxy/#setting-up-apache-httpd-in-front-of-tomcat","text":"I prefer, for a number of reasons, to put Apache httpd in front of Tomcat (even though there's a load balancer in front of the CAS servers as well. I'm sure there's ways around this, but I like the flexibility that it provides me, and I also have had other applications on the CAS servers that are not running on Tomcat and still need to be accessible (including SimpleSAML for applications we haven't moved to Azure or directly into CAS yet). For now, I'm not going to go over full configuration of Apache httpd here. I will eventually (hopefully by end of March 2021) put up my steps and Ansible playbooks to get httpd running, but for now - I'll mention the Tomcat/httpd specific portion. We use AJP for communication between Apache httpd and Apache Tomcat. Configuring this is in two steps, one on the httpd server and one on Tomcat.","title":"Setting up Apache HTTPD in front of Tomcat"},{"location":"setting-up-the-environment/httpd/ajp-proxy/#on-the-httpd-server","text":"Create a configuration file in /etc/httpd/conf.d. We call ours 'cas-ajp.conf' but it doesn't matter as long as it ends in .conf. The contents of which are below: ProxyRequests Off <Proxy *> Order allow,deny Allow from all </Proxy> ProxyPass /cas ajp://localhost:8009/cas ProxyPassReverse /cas ajp://localhost:8009/cas","title":"On the httpd server"},{"location":"setting-up-the-environment/httpd/ajp-proxy/#on-the-tomcat-server","text":"Edit /etc/tomcat/server.xml and define an AJP port: <!-- Define an AJP 1.3 Connector on port 8009 See https://tomcat.apache.org/tomcat-9.0-doc/config/ajp.html for more on the 'secretRequired' and 'secret' options. Since I'm only exposing this to localhost via host firewall --> <Connector port= \"8009\" protocol= \"AJP/1.3\" redirectPort= \"8443\" secretRequired= \"false\" /> The Tomcat portion is already in the Tomcat server.xml that is linked to from the Tomcat/Ansible section.","title":"On the Tomcat server"},{"location":"setting-up-the-environment/tomcat/asynch-request-support/","text":"Configure asynchronous request support Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. With Ansible, the web.xml file will be a template pushed from Ansible. CAS 6's documentation states \"In the event that an external servlet container is used, you MAY need to make sure it\u2019s configured correctly to support asynchronous requests in the event you get related errors and your container requires this.\" . Set async-supported to true within default servlet To implement this edit /etc/tomcat/web.xml and look for the definition of the default web app servlet, around line 113. You'll want to add the async-supported directive before the end of the servlet. <!-- Before changes --> <servlet> <servlet-name> default </servlet-name> <servlet-class> org.apache.catalina.servlets.DefaultServlet </servlet-class> <init-param> <param-name> debug </param-name> <param-value> 0 </param-value> </init-param> <init-param> <param-name> listings </param-name> <param-value> false </param-value> </init-param> <load-on-startup> 1 </load-on-startup> </servlet> <!-- After changes --> <servlet> <servlet-name> default </servlet-name> <servlet-class> org.apache.catalina.servlets.DefaultServlet </servlet-class> <init-param> <param-name> debug </param-name> <param-value> 0 </param-value> </init-param> <init-param> <param-name> listings </param-name> <param-value> false </param-value> </init-param> <load-on-startup> 1 </load-on-startup> <async-supported> true </async-supported> </servlet> Set async-supported to true within jsp compiler and execution servlet To implement this edit /etc/tomcat/web.xml and look for the definition of the jsp servlet, around line 268. You'll want to add the async-supported directive before the end of the servlet. <!-- Before changes --> <servlet> <servlet-name> jsp </servlet-name> <servlet-class> org.apache.jasper.servlet.JspServlet </servlet-class> <init-param> <param-name> fork </param-name> <param-value> false </param-value> </init-param> <init-param> <param-name> xpoweredBy </param-name> <param-value> false </param-value> </init-param> <load-on-startup> 3 </load-on-startup> </servlet> <!-- Before changes --> <servlet> <servlet-name> jsp </servlet-name> <servlet-class> org.apache.jasper.servlet.JspServlet </servlet-class> <init-param> <param-name> fork </param-name> <param-value> false </param-value> </init-param> <init-param> <param-name> xpoweredBy </param-name> <param-value> false </param-value> </init-param> <load-on-startup> 3 </load-on-startup> <async-supported> true </async-supported> </servlet> References CAS 6: Servlet Container Configuration","title":"Configure asynchronous request support"},{"location":"setting-up-the-environment/tomcat/asynch-request-support/#configure-asynchronous-request-support","text":"Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. With Ansible, the web.xml file will be a template pushed from Ansible. CAS 6's documentation states \"In the event that an external servlet container is used, you MAY need to make sure it\u2019s configured correctly to support asynchronous requests in the event you get related errors and your container requires this.\" .","title":"Configure asynchronous request support"},{"location":"setting-up-the-environment/tomcat/asynch-request-support/#set-async-supported-to-true-within-default-servlet","text":"To implement this edit /etc/tomcat/web.xml and look for the definition of the default web app servlet, around line 113. You'll want to add the async-supported directive before the end of the servlet. <!-- Before changes --> <servlet> <servlet-name> default </servlet-name> <servlet-class> org.apache.catalina.servlets.DefaultServlet </servlet-class> <init-param> <param-name> debug </param-name> <param-value> 0 </param-value> </init-param> <init-param> <param-name> listings </param-name> <param-value> false </param-value> </init-param> <load-on-startup> 1 </load-on-startup> </servlet> <!-- After changes --> <servlet> <servlet-name> default </servlet-name> <servlet-class> org.apache.catalina.servlets.DefaultServlet </servlet-class> <init-param> <param-name> debug </param-name> <param-value> 0 </param-value> </init-param> <init-param> <param-name> listings </param-name> <param-value> false </param-value> </init-param> <load-on-startup> 1 </load-on-startup> <async-supported> true </async-supported> </servlet>","title":"Set async-supported to true within default servlet"},{"location":"setting-up-the-environment/tomcat/asynch-request-support/#set-async-supported-to-true-within-jsp-compiler-and-execution-servlet","text":"To implement this edit /etc/tomcat/web.xml and look for the definition of the jsp servlet, around line 268. You'll want to add the async-supported directive before the end of the servlet. <!-- Before changes --> <servlet> <servlet-name> jsp </servlet-name> <servlet-class> org.apache.jasper.servlet.JspServlet </servlet-class> <init-param> <param-name> fork </param-name> <param-value> false </param-value> </init-param> <init-param> <param-name> xpoweredBy </param-name> <param-value> false </param-value> </init-param> <load-on-startup> 3 </load-on-startup> </servlet> <!-- Before changes --> <servlet> <servlet-name> jsp </servlet-name> <servlet-class> org.apache.jasper.servlet.JspServlet </servlet-class> <init-param> <param-name> fork </param-name> <param-value> false </param-value> </init-param> <init-param> <param-name> xpoweredBy </param-name> <param-value> false </param-value> </init-param> <load-on-startup> 3 </load-on-startup> <async-supported> true </async-supported> </servlet>","title":"Set async-supported to true within jsp compiler and execution servlet"},{"location":"setting-up-the-environment/tomcat/asynch-request-support/#references","text":"CAS 6: Servlet Container Configuration","title":"References"},{"location":"setting-up-the-environment/tomcat/config-x-forwarded/","text":"Configure X-Forwarded-For header processing Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. With Ansible, the server.xml file will be a template pushed from Ansible. You will most likely have CAS behind a load balancer. If you have things like we have them in our environment, the source IP that comes to the web server is the IP of the load balancer, not the actual client IP. Most, if not all load balancers can be configured to insert an X-Forwarded-For HTTP header to identify the address of the connecting system. Tomcat can be configured as follows to look for this header and use it instead of the load balancer's source IP. To configure Tomcat to process X-Forwarded-For HTTP headers, edit the file /etc/tomcat/server.xml and locate the definition of the AccessLogValve (around line 164, after inserting the changes in Configure TLS/SSL settings) and Insert a RemoteIpValve definition above it. Add a requestAttributesEnabled attribute to the AccessLogValve definition. <!--Before changes--> <!-- Access log processes all example. Documentation at: /docs/config/valve.html Note: The pattern used is equivalent to using pattern=\"common\" --> <Valve className= \"org.apache.catalina.valves.AccessLogValve\" directory= \"logs\" prefix= \"localhost_access_log\" suffix= \".txt\" pattern= \"%h %l %u %t &quot;%r&quot; %s %b\" /> <!-- After changes--> <!-- RemoteIp valve, process X-Forwarded-For headers Documentation at: /docs/config/valve.html IP addresses below are those your load balancer uses to talk to your application server. Multiple IP addresses separated by | --> <Valve className= \"org.apache.catalina.valves.RemoteIpValve\" internalProxies= \"192\\.168\\.1\\.10|192\\.168\\.1\\.11\" /> <!-- Access log processes all example. Documentation at: /docs/config/valve.html Note: The pattern used is equivalent to using pattern=\"common\" --> <Valve className= \"org.apache.catalina.valves.AccessLogValve\" directory= \"logs\" prefix= \"localhost_access_log\" suffix= \".txt\" requestAttributesEnabled= \"true\" pattern= \"%h %l %u %t &quot;%r&quot; %s %b\" />","title":"Configure X-Forwarded-For header processing"},{"location":"setting-up-the-environment/tomcat/config-x-forwarded/#configure-x-forwarded-for-header-processing","text":"Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. With Ansible, the server.xml file will be a template pushed from Ansible. You will most likely have CAS behind a load balancer. If you have things like we have them in our environment, the source IP that comes to the web server is the IP of the load balancer, not the actual client IP. Most, if not all load balancers can be configured to insert an X-Forwarded-For HTTP header to identify the address of the connecting system. Tomcat can be configured as follows to look for this header and use it instead of the load balancer's source IP. To configure Tomcat to process X-Forwarded-For HTTP headers, edit the file /etc/tomcat/server.xml and locate the definition of the AccessLogValve (around line 164, after inserting the changes in Configure TLS/SSL settings) and Insert a RemoteIpValve definition above it. Add a requestAttributesEnabled attribute to the AccessLogValve definition. <!--Before changes--> <!-- Access log processes all example. Documentation at: /docs/config/valve.html Note: The pattern used is equivalent to using pattern=\"common\" --> <Valve className= \"org.apache.catalina.valves.AccessLogValve\" directory= \"logs\" prefix= \"localhost_access_log\" suffix= \".txt\" pattern= \"%h %l %u %t &quot;%r&quot; %s %b\" /> <!-- After changes--> <!-- RemoteIp valve, process X-Forwarded-For headers Documentation at: /docs/config/valve.html IP addresses below are those your load balancer uses to talk to your application server. Multiple IP addresses separated by | --> <Valve className= \"org.apache.catalina.valves.RemoteIpValve\" internalProxies= \"192\\.168\\.1\\.10|192\\.168\\.1\\.11\" /> <!-- Access log processes all example. Documentation at: /docs/config/valve.html Note: The pattern used is equivalent to using pattern=\"common\" --> <Valve className= \"org.apache.catalina.valves.AccessLogValve\" directory= \"logs\" prefix= \"localhost_access_log\" suffix= \".txt\" requestAttributesEnabled= \"true\" pattern= \"%h %l %u %t &quot;%r&quot; %s %b\" />","title":"Configure X-Forwarded-For header processing"},{"location":"setting-up-the-environment/tomcat/configure-async-logging/","text":"Configure asynchronous logging support Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. With Ansible, the catalina.properties file will be a template pushed from Ansible. CAS 6\u2019s logging subsystem automatically inserts itself into the runtime application context at startup time, and is designed to clean up the logging context when Tomcat shuts down. Unfortunately, the default Tomcat JarScanner configuration skips over JAR files named log4j*.jar, which prevents this feature from working. To correct this problem, edit the file /etc/tomcat/catalina.properties and locate the lines defining the jarsToSkip property (starting around line 108), and then the specific line of that definition that includes log4j*.jar (around line 161): tomcat.util.scan.StandardJarScanFilter.jarsToSkip=\\ annotations-api.jar,\\ ant-junit*.jar,\\ ant-launcher.jar,\\ (snip junit.jar,\\ log4j*.jar,\\ mail*.jar,\\ (snip) xmlParserAPIs.jar,\\ xom-*.jar Remove the log4j*.jar, \\ line completely.","title":"Configure asynchronous logging support"},{"location":"setting-up-the-environment/tomcat/configure-async-logging/#configure-asynchronous-logging-support","text":"Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. With Ansible, the catalina.properties file will be a template pushed from Ansible. CAS 6\u2019s logging subsystem automatically inserts itself into the runtime application context at startup time, and is designed to clean up the logging context when Tomcat shuts down. Unfortunately, the default Tomcat JarScanner configuration skips over JAR files named log4j*.jar, which prevents this feature from working. To correct this problem, edit the file /etc/tomcat/catalina.properties and locate the lines defining the jarsToSkip property (starting around line 108), and then the specific line of that definition that includes log4j*.jar (around line 161): tomcat.util.scan.StandardJarScanFilter.jarsToSkip=\\ annotations-api.jar,\\ ant-junit*.jar,\\ ant-launcher.jar,\\ (snip junit.jar,\\ log4j*.jar,\\ mail*.jar,\\ (snip) xmlParserAPIs.jar,\\ xom-*.jar Remove the log4j*.jar, \\ line completely.","title":"Configure asynchronous logging support"},{"location":"setting-up-the-environment/tomcat/entropy-daemon/","text":"Install an entropy daemon Note This step is recommended if running CAS on virtual Linux servers. It is not necessary if running CAS on physical Linux servers or Windows servers of either type. A common problem on virtual Linux servers is that the /dev/random device will run low on entropy, because most of the sources the kernel uses to build up the entropy pool are hardware-based, and therefore do not exist in a virtual environment. If there\u2019s not enough entropy available when Tomcat is started, it can often take two or three minutes or longer for the server to start. Once Tomcat has started and the CAS application has been loaded, entropy is still required to establish secure (HTTPS) connections with authenticating users\u2019 browsers and protected applications. A lack of available entropy will adversely affect the performance of the application by limiting the rate at which connections can be processed. To improve the size of the entropy pool on Linux, it\u2019s possible to feed random data from an external source into /dev/random. One way to do this is the haveged daemon, which uses the HAVEGE (HArdware Volatile Entropy Gathering and Expansion) algorithm to harvest the indirect effects of hardware events on hidden processor state (caches, branch predictors, memory translation tables, etc) to generate random bytes with which to fill /dev/random whenever the supply of random bits falls below the low water mark of the device. We will use this approach to avoid entropy depletion on the CAS servers. Red Hat does not offer haveged on RHEL 7, but it can be installed from the Fedora Project\u2019s Extra Packages for Enterprise Linux (EPEL) repository. I typically find at least ONE package I need from EPEL on each server I'm involved in, so it's part of my normal build process. Install the EPEL repository HAVEGE is needed on each CAS server, and since it needs EPEL, that is needed on each CAS server as well. dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm !!! note: According to EPEL: \"on RHEL 8 it is required to also enable the codeready-builder-for-rhel-8-*-rpms repository since EPEL packages may depend on packages from it\". I haven't needed it for the installed packages, but to be safe - I recommend following their guidance. # Installation commands for codeready-building are below. ARCH=$( /bin/arch ) subscription-manager repos --enable \"codeready-builder-for-rhel-8-${ARCH}-rpms\" # Note: since all our servers here are x86_64, I just run the following: subscription-manager repos --enable \"codeready-builder-for-rhel-8-x86_64-rpms\" Install, enable, and start haveged # Install haveged dnf install haveged # Enable the service systemctl enable haveged # Start the service systemctl start haveged # Verify that it is running systemctl status haveged References Extra Packages for Enterprise Linux (EPEL) haveged - A simple entropy daemon","title":"Install an entropy daemon"},{"location":"setting-up-the-environment/tomcat/entropy-daemon/#install-an-entropy-daemon","text":"Note This step is recommended if running CAS on virtual Linux servers. It is not necessary if running CAS on physical Linux servers or Windows servers of either type. A common problem on virtual Linux servers is that the /dev/random device will run low on entropy, because most of the sources the kernel uses to build up the entropy pool are hardware-based, and therefore do not exist in a virtual environment. If there\u2019s not enough entropy available when Tomcat is started, it can often take two or three minutes or longer for the server to start. Once Tomcat has started and the CAS application has been loaded, entropy is still required to establish secure (HTTPS) connections with authenticating users\u2019 browsers and protected applications. A lack of available entropy will adversely affect the performance of the application by limiting the rate at which connections can be processed. To improve the size of the entropy pool on Linux, it\u2019s possible to feed random data from an external source into /dev/random. One way to do this is the haveged daemon, which uses the HAVEGE (HArdware Volatile Entropy Gathering and Expansion) algorithm to harvest the indirect effects of hardware events on hidden processor state (caches, branch predictors, memory translation tables, etc) to generate random bytes with which to fill /dev/random whenever the supply of random bits falls below the low water mark of the device. We will use this approach to avoid entropy depletion on the CAS servers. Red Hat does not offer haveged on RHEL 7, but it can be installed from the Fedora Project\u2019s Extra Packages for Enterprise Linux (EPEL) repository. I typically find at least ONE package I need from EPEL on each server I'm involved in, so it's part of my normal build process.","title":"Install an entropy daemon"},{"location":"setting-up-the-environment/tomcat/entropy-daemon/#install-the-epel-repository","text":"HAVEGE is needed on each CAS server, and since it needs EPEL, that is needed on each CAS server as well. dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm !!! note: According to EPEL: \"on RHEL 8 it is required to also enable the codeready-builder-for-rhel-8-*-rpms repository since EPEL packages may depend on packages from it\". I haven't needed it for the installed packages, but to be safe - I recommend following their guidance. # Installation commands for codeready-building are below. ARCH=$( /bin/arch ) subscription-manager repos --enable \"codeready-builder-for-rhel-8-${ARCH}-rpms\" # Note: since all our servers here are x86_64, I just run the following: subscription-manager repos --enable \"codeready-builder-for-rhel-8-x86_64-rpms\"","title":"Install the EPEL repository"},{"location":"setting-up-the-environment/tomcat/entropy-daemon/#install-enable-and-start-haveged","text":"# Install haveged dnf install haveged # Enable the service systemctl enable haveged # Start the service systemctl start haveged # Verify that it is running systemctl status haveged","title":"Install, enable, and start haveged"},{"location":"setting-up-the-environment/tomcat/entropy-daemon/#references","text":"Extra Packages for Enterprise Linux (EPEL) haveged - A simple entropy daemon","title":"References"},{"location":"setting-up-the-environment/tomcat/java/","text":"Install Java CAS 6 requires Java 11 or later. Due to the licensing changes from Oracle, I have only used OpenJDK for the last several years. If you have the licensing from Oracle and prefer their version, go ahead! Likewise there's also AdoptOpenJDK but I only have direct experience using OpenJDK with CAS 5 or 6. OpenJDK has the advantage of being directly in the RHEL repositories, so it's one less thing to manage outside of the OS package management. This is needed on the build servers, and all the CAS servers. dnf install java-11-openjdk java-11-openjdk-devel You can have multiple Java versions installed, but you will want to have JDK 11 as the default (or will have to make sure it's the default for Tomcat at least). To check which is the default, use: java --version","title":"Install Java"},{"location":"setting-up-the-environment/tomcat/java/#install-java","text":"CAS 6 requires Java 11 or later. Due to the licensing changes from Oracle, I have only used OpenJDK for the last several years. If you have the licensing from Oracle and prefer their version, go ahead! Likewise there's also AdoptOpenJDK but I only have direct experience using OpenJDK with CAS 5 or 6. OpenJDK has the advantage of being directly in the RHEL repositories, so it's one less thing to manage outside of the OS package management. This is needed on the build servers, and all the CAS servers. dnf install java-11-openjdk java-11-openjdk-devel You can have multiple Java versions installed, but you will want to have JDK 11 as the default (or will have to make sure it's the default for Tomcat at least). To check which is the default, use: java --version","title":"Install Java"},{"location":"setting-up-the-environment/tomcat/resource-caching/","text":"Tune resource caching settings Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. With Ansible, the context.xml file will be a template pushed from Ansible. To improve performance, Tomcat is configured by default to cache static resources. However, the size of the cache is too small to work effectively with the CAS application. To tune Tomcat\u2019s cache settings, edit the file /etc/tomcat/context.xml , locate the definition of the default context (around line 19), and add a directive at the bottom: <!--Before changes--> <Context> <!-- Default set of monitored resources. If one of these changes, the --> <!-- web application will be reloaded. --> <WatchedResource> WEB-INF/web.xml </WatchedResource> <WatchedResource> WEB-INF/tomcat-web.xml </WatchedResource> <WatchedResource> ${catalina.base}/conf/web.xml </WatchedResource> <!-- Uncomment this to disable session persistence across Tomcat restarts --> <!-- <Manager pathname=\"\" /> --> </Context> <!--After changes--> <Context> <!-- Default set of monitored resources. If one of these changes, the --> <!-- web application will be reloaded. --> <WatchedResource> WEB-INF/web.xml </WatchedResource> <WatchedResource> WEB-INF/tomcat-web.xml </WatchedResource> <WatchedResource> ${catalina.base}/conf/web.xml </WatchedResource> <!-- Uncomment this to disable session persistence across Tomcat restarts --> <!-- <Manager pathname=\"\" /> --> <Resources cachingAllowed= \"true\" cacheMaxSize= \"40960\" cacheTtl= \"60000\" /> </Context>","title":"Tune resource caching settings"},{"location":"setting-up-the-environment/tomcat/resource-caching/#tune-resource-caching-settings","text":"Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. With Ansible, the context.xml file will be a template pushed from Ansible. To improve performance, Tomcat is configured by default to cache static resources. However, the size of the cache is too small to work effectively with the CAS application. To tune Tomcat\u2019s cache settings, edit the file /etc/tomcat/context.xml , locate the definition of the default context (around line 19), and add a directive at the bottom: <!--Before changes--> <Context> <!-- Default set of monitored resources. If one of these changes, the --> <!-- web application will be reloaded. --> <WatchedResource> WEB-INF/web.xml </WatchedResource> <WatchedResource> WEB-INF/tomcat-web.xml </WatchedResource> <WatchedResource> ${catalina.base}/conf/web.xml </WatchedResource> <!-- Uncomment this to disable session persistence across Tomcat restarts --> <!-- <Manager pathname=\"\" /> --> </Context> <!--After changes--> <Context> <!-- Default set of monitored resources. If one of these changes, the --> <!-- web application will be reloaded. --> <WatchedResource> WEB-INF/web.xml </WatchedResource> <WatchedResource> WEB-INF/tomcat-web.xml </WatchedResource> <WatchedResource> ${catalina.base}/conf/web.xml </WatchedResource> <!-- Uncomment this to disable session persistence across Tomcat restarts --> <!-- <Manager pathname=\"\" /> --> <Resources cachingAllowed= \"true\" cacheMaxSize= \"40960\" cacheTtl= \"60000\" /> </Context>","title":"Tune resource caching settings"},{"location":"setting-up-the-environment/tomcat/systemd-service/","text":"Configure systemd to start Tomcat Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. With Ansible, the unit file will be an Ansible template that it will push out and enable. RHEL, since version 7, has used systemd (instead of old sysv init scripts) to manage system resources. A unit in systemd is any resource systemd knows how to operate on and manage. This will give you a basic unit file with the ability to start/stop/restart the service (including on boot). Definte Tomcat as a service unit Create a new file: /etc/systemd/system/tomcat.service with the contents below: [Unit] Description=Apache Tomcat Web Application Container After=network.target [Service] Type=forking PIDFile=/var/run/tomcat.pid UMask=0007 # Tomcat variables Environment='JAVA_HOME=/usr/lib/jvm/java-openjdk' Environment='CATALINA_PID=/var/run/tomcat.pid' Environment='CATALINA_HOME=/opt/tomcat/latest' Environment='CATALINA_BASE=/opt/tomcat/latest' Environment='CATALINA_OPTS=-Xms512M -Xmx2048M -XX:+UseParallelGC -server' # Needed to make use of Tomcat Native Library Environment='LD_LIBRARY_PATH=/opt/tomcat/latest/lib' ExecStart=/opt/tomcat/latest/bin/jsvc \\ -Dcatalina.home=${CATALINA_HOME} \\ -Dcatalina.base=${CATALINA_BASE} \\ -Djava.awt.headless=true \\ -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager \\ -Djava.util.logging.config.file=${CATALINA_BASE}/conf/logging.properties \\ -cp ${CATALINA_HOME}/bin/commons-daemon.jar:${CATALINA_HOME}/bin/bootstrap.jar:${CATALINA_HOME}/bin/tomcat-juli.jar \\ -pidfile ${CATALINA_PID} \\ -java-home ${JAVA_HOME} \\ -user tomcat \\ $CATALINA_OPTS \\ org.apache.catalina.startup.Bootstrap ExecStop=/opt/tomcat/latest/bin/jsvc \\ -pidfile ${CATALINA_PID} \\ -stop \\ org.apache.catalina.startup.Bootstrap [Install] WantedBy=multi-user.target Enable the Tomcat service Set the appropriate SELinux file context and file permissions: restorecon /etc/systemd/system/tomcat.service chmod 644 /etc/systemd/system/tomcat.service Enable the service - which will cause Tomcat to be started at boot. systemctl enable tomcat.service You can then stop/start/restart and check the status of Tomcat systemctl start tomcat systemctl stop tomcat systemctl restart tomcat systemctl status tomcat","title":"Configure systemd to start tomcat"},{"location":"setting-up-the-environment/tomcat/systemd-service/#configure-systemd-to-start-tomcat","text":"Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. With Ansible, the unit file will be an Ansible template that it will push out and enable. RHEL, since version 7, has used systemd (instead of old sysv init scripts) to manage system resources. A unit in systemd is any resource systemd knows how to operate on and manage. This will give you a basic unit file with the ability to start/stop/restart the service (including on boot).","title":"Configure systemd to start Tomcat"},{"location":"setting-up-the-environment/tomcat/systemd-service/#definte-tomcat-as-a-service-unit","text":"Create a new file: /etc/systemd/system/tomcat.service with the contents below: [Unit] Description=Apache Tomcat Web Application Container After=network.target [Service] Type=forking PIDFile=/var/run/tomcat.pid UMask=0007 # Tomcat variables Environment='JAVA_HOME=/usr/lib/jvm/java-openjdk' Environment='CATALINA_PID=/var/run/tomcat.pid' Environment='CATALINA_HOME=/opt/tomcat/latest' Environment='CATALINA_BASE=/opt/tomcat/latest' Environment='CATALINA_OPTS=-Xms512M -Xmx2048M -XX:+UseParallelGC -server' # Needed to make use of Tomcat Native Library Environment='LD_LIBRARY_PATH=/opt/tomcat/latest/lib' ExecStart=/opt/tomcat/latest/bin/jsvc \\ -Dcatalina.home=${CATALINA_HOME} \\ -Dcatalina.base=${CATALINA_BASE} \\ -Djava.awt.headless=true \\ -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager \\ -Djava.util.logging.config.file=${CATALINA_BASE}/conf/logging.properties \\ -cp ${CATALINA_HOME}/bin/commons-daemon.jar:${CATALINA_HOME}/bin/bootstrap.jar:${CATALINA_HOME}/bin/tomcat-juli.jar \\ -pidfile ${CATALINA_PID} \\ -java-home ${JAVA_HOME} \\ -user tomcat \\ $CATALINA_OPTS \\ org.apache.catalina.startup.Bootstrap ExecStop=/opt/tomcat/latest/bin/jsvc \\ -pidfile ${CATALINA_PID} \\ -stop \\ org.apache.catalina.startup.Bootstrap [Install] WantedBy=multi-user.target","title":"Definte Tomcat as a service unit"},{"location":"setting-up-the-environment/tomcat/systemd-service/#enable-the-tomcat-service","text":"Set the appropriate SELinux file context and file permissions: restorecon /etc/systemd/system/tomcat.service chmod 644 /etc/systemd/system/tomcat.service Enable the service - which will cause Tomcat to be started at boot. systemctl enable tomcat.service You can then stop/start/restart and check the status of Tomcat systemctl start tomcat systemctl stop tomcat systemctl restart tomcat systemctl status tomcat","title":"Enable the Tomcat service"},{"location":"setting-up-the-environment/tomcat/tomcat-dependencies/","text":"Tomcat Dependencies Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. As discussed in the Tomcat Overview , we will be installing: Apache Portable Runtime (APR) - as per Apache, \"Tomcat can use the Apache Portable Runtime to provide superior scalability, performance, and better integration with native server technologies\" Tomcat Native Library - this is best described on the Apache site as \"an optional component for use with Apache Tomcat that allows Tomcat to use certain native resources for performance, compatibility, etc.\" Apache Commons Daemon - this allows Apache to be started as root to perform some privileged operations, then switch to a non-privileged user. The Tomcat Native Library and Commons Daemon are shipped with Tomcat but need to be compiled. APR is downloaded and installed separately from Apache. Apache Portable Runtime The Tomcat Native Library, which will be installed later here, depends on the Apache Portable Runtime (APR) library. As with Tomcat itself, we'll be installing APR within /opt/apr - with a symlink to the latest version. To download, compile, and install this, see the following: Note Your URL in the wget command below will differ depending on which mirror you are getting from the APR Download page, and which specific version of APR you are downloading (1.7.0 was released in April 2019 and it does not change in version nearly as much as Apache Tomcat). Replace 1.7.0 with the latest version. [root@login6devb ~]# mkdir -p /opt/apr/apr-1.7.0 [root@login6devb ~]# ln -s /opt/apr/apr-1.7.0 /opt/apr/latest [root@login6devb ~]# cd /tmp/ [root@login6devb tmp]# wget https://mirrors.ocf.berkeley.edu/apache//apr/apr-1.7.0.tar.gz (download progress will show) [root@login6devb tmp]# tar xzf apr-1.7.0.tar.gz (There will be a lot of output - look for errors. There may be an error of \"rm: cannot remove 'libtoolT': No such file or directory\" which appears harmless. If you get an error like 'no acceptable C compiler found in $PATH' - make sure gcc is installed. This is listed in the prerequisies section of the tomcat-overview.) [root@login6devb tmp]# cd apr-1.7.0 [root@login6devb apr-1.7.0]# make (There will be a lot of output - look for errors.) [root@login6devb apr-1.7.0]# make test (There will be a lot of output and it may take a while to run - look for errors.) [root@login6devb apr-1.7.0]# make install (There will be a lot of output - look for errors.) [root@login6devb tmp]# rm -rf /tmp/apr-1.7.0 /tmp/apr-1.7.0.tar.gz When it's done - you'll have /opt/apr/apr-1.7.0 (or whatever version) installed and with /opt/apr/latest as a symlink to the latest version. This way you can update APR to a later version without having to recompile other applications that use it. Tomcat Native Library The source for Tomcat Native Library is included in the Tomcat version that was downloaded in the previous page. It has to be extracted, compiled, and installed. Note: The tomcat native version may change between Tomcat minor releases. For example, it is 1.2.26 as of Tomcat 9.0.43 . To do so: [root@login6devb ~]# cd /opt/tomcat/latest/bin/ [root@login6devb bin]# tar xzf tomcat-native.tar.gz [root@login6devb bin]# cd tomcat-native-*-src/native [root@login6devb native]# ./configure \\ > --with-java-home = /usr/lib/jvm/java-openjdk \\ > --with-apr = /opt/apr/latest/bin/apr-1-config \\ > --prefix = /opt/tomcat/apache-tomcat-9.0.43 (There will be a lot of output - look for errors.) [root@login6devb native]# make (There will be a lot of output - look for errors. You're probably tired of me saying this though!) [root@login6devb native]# make install (There will be a lot of output - look for errors.) [root@login6devb native]# cd ../.. [root@login6devb bin]# rm -rf tomcat-native-*-src When done - the Tomcat Native Library will be installed in the lib directory of the version of Tomcat you're working in. You can see the 'libtcnative' files there. Apache Commons Daemon (jsvc) The Apache Commons Daemon (jsvc) allows Tomcat to be started as root to perform some privileged operations (such as binding to ports below 1024) and then switch identity to run as a non-privileged user, which is better from a security perspective. Like the Tomcat Native Library, the Commons Daemon is included as part of the Tomcat distribution; it just needs to be extracted, compiled, and installed. Note: Like the tomcat native, library, the apache commons daemon version may change between Tomcat minor releases. For example, it is 1.2.26 as of Tomcat 9.0.43 . To do so: [root@login6devb ~]# cd /opt/tomcat/latest/bin/ [root@login6devb bin]# tar xzf commons-daemon-native.tar.gz [root@login6devb bin]# cd commons-daemon-*-native-src/unix [root@login6devb unix]# ./configure --with-java = /usr/lib/jvm/java-openjdk (There will be a lot of output - look for errors.) [root@login6devb unix]# make [root@login6devb unix]# mv jsvc ../.. [root@login6devb unix]# cd ../.. [root@login6devb unix]# rm -rf commons-daemon-*-native-src This installs the jsvc binary within the bin directory of the version of Tomcat you're working in.","title":"Install Tomcat dependencies"},{"location":"setting-up-the-environment/tomcat/tomcat-dependencies/#tomcat-dependencies","text":"Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. As discussed in the Tomcat Overview , we will be installing: Apache Portable Runtime (APR) - as per Apache, \"Tomcat can use the Apache Portable Runtime to provide superior scalability, performance, and better integration with native server technologies\" Tomcat Native Library - this is best described on the Apache site as \"an optional component for use with Apache Tomcat that allows Tomcat to use certain native resources for performance, compatibility, etc.\" Apache Commons Daemon - this allows Apache to be started as root to perform some privileged operations, then switch to a non-privileged user. The Tomcat Native Library and Commons Daemon are shipped with Tomcat but need to be compiled. APR is downloaded and installed separately from Apache.","title":"Tomcat Dependencies"},{"location":"setting-up-the-environment/tomcat/tomcat-dependencies/#apache-portable-runtime","text":"The Tomcat Native Library, which will be installed later here, depends on the Apache Portable Runtime (APR) library. As with Tomcat itself, we'll be installing APR within /opt/apr - with a symlink to the latest version. To download, compile, and install this, see the following: Note Your URL in the wget command below will differ depending on which mirror you are getting from the APR Download page, and which specific version of APR you are downloading (1.7.0 was released in April 2019 and it does not change in version nearly as much as Apache Tomcat). Replace 1.7.0 with the latest version. [root@login6devb ~]# mkdir -p /opt/apr/apr-1.7.0 [root@login6devb ~]# ln -s /opt/apr/apr-1.7.0 /opt/apr/latest [root@login6devb ~]# cd /tmp/ [root@login6devb tmp]# wget https://mirrors.ocf.berkeley.edu/apache//apr/apr-1.7.0.tar.gz (download progress will show) [root@login6devb tmp]# tar xzf apr-1.7.0.tar.gz (There will be a lot of output - look for errors. There may be an error of \"rm: cannot remove 'libtoolT': No such file or directory\" which appears harmless. If you get an error like 'no acceptable C compiler found in $PATH' - make sure gcc is installed. This is listed in the prerequisies section of the tomcat-overview.) [root@login6devb tmp]# cd apr-1.7.0 [root@login6devb apr-1.7.0]# make (There will be a lot of output - look for errors.) [root@login6devb apr-1.7.0]# make test (There will be a lot of output and it may take a while to run - look for errors.) [root@login6devb apr-1.7.0]# make install (There will be a lot of output - look for errors.) [root@login6devb tmp]# rm -rf /tmp/apr-1.7.0 /tmp/apr-1.7.0.tar.gz When it's done - you'll have /opt/apr/apr-1.7.0 (or whatever version) installed and with /opt/apr/latest as a symlink to the latest version. This way you can update APR to a later version without having to recompile other applications that use it.","title":"Apache Portable Runtime"},{"location":"setting-up-the-environment/tomcat/tomcat-dependencies/#tomcat-native-library","text":"The source for Tomcat Native Library is included in the Tomcat version that was downloaded in the previous page. It has to be extracted, compiled, and installed. Note: The tomcat native version may change between Tomcat minor releases. For example, it is 1.2.26 as of Tomcat 9.0.43 . To do so: [root@login6devb ~]# cd /opt/tomcat/latest/bin/ [root@login6devb bin]# tar xzf tomcat-native.tar.gz [root@login6devb bin]# cd tomcat-native-*-src/native [root@login6devb native]# ./configure \\ > --with-java-home = /usr/lib/jvm/java-openjdk \\ > --with-apr = /opt/apr/latest/bin/apr-1-config \\ > --prefix = /opt/tomcat/apache-tomcat-9.0.43 (There will be a lot of output - look for errors.) [root@login6devb native]# make (There will be a lot of output - look for errors. You're probably tired of me saying this though!) [root@login6devb native]# make install (There will be a lot of output - look for errors.) [root@login6devb native]# cd ../.. [root@login6devb bin]# rm -rf tomcat-native-*-src When done - the Tomcat Native Library will be installed in the lib directory of the version of Tomcat you're working in. You can see the 'libtcnative' files there.","title":"Tomcat Native Library"},{"location":"setting-up-the-environment/tomcat/tomcat-dependencies/#apache-commons-daemon-jsvc","text":"The Apache Commons Daemon (jsvc) allows Tomcat to be started as root to perform some privileged operations (such as binding to ports below 1024) and then switch identity to run as a non-privileged user, which is better from a security perspective. Like the Tomcat Native Library, the Commons Daemon is included as part of the Tomcat distribution; it just needs to be extracted, compiled, and installed. Note: Like the tomcat native, library, the apache commons daemon version may change between Tomcat minor releases. For example, it is 1.2.26 as of Tomcat 9.0.43 . To do so: [root@login6devb ~]# cd /opt/tomcat/latest/bin/ [root@login6devb bin]# tar xzf commons-daemon-native.tar.gz [root@login6devb bin]# cd commons-daemon-*-native-src/unix [root@login6devb unix]# ./configure --with-java = /usr/lib/jvm/java-openjdk (There will be a lot of output - look for errors.) [root@login6devb unix]# make [root@login6devb unix]# mv jsvc ../.. [root@login6devb unix]# cd ../.. [root@login6devb unix]# rm -rf commons-daemon-*-native-src This installs the jsvc binary within the bin directory of the version of Tomcat you're working in.","title":"Apache Commons Daemon (jsvc)"},{"location":"setting-up-the-environment/tomcat/tomcat-harden/","text":"Harden the installation Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. The Tomcat Security Considerations document makes several recommendations for hardening a Tomcat installation: Tomcat should not be run as the root user; it should be run as a dedicated user (usually named tomcat) that has minimum operating system permissions. It should not be possible to log in remotely as the tomcat user. All Tomcat files should be owned by user root and group tomcat (the tomcat user\u2019s default group should be group tomcat). File/directory permissions should be set to owner read/write, group read only, and world none. The exceptions are the logs, temp, and work directories, which should be owned by the tomcat user instead of root. The default and example web applications included with the Tomcat distribution should be removed if they are not needed. Auto-deployment should be disabled, and web applications should be deployed as exploded directories rather than web application archives (WAR files). Implementing these recommendations means that, even if an attacker compromises the Tomcat process, he or she cannot change the Tomcat configuration, deploy new web applications, or modify existing web applications. Create a tomcat user and group [root@login6devb ~]# groupadd -r tomcat [root@login6devb ~]# useradd -r -d /opt/tomcat -g tomcat -s /sbin/nologin tomcat (This gives a 'shell' of /sbin/nologin which prevents the user from logging in) Set file ownership and permissions Some of these should be owned by user:root and group:tomcat (conf and webapps). Others (logs temp work) should be owned by user/group:tomcat. mkdir -p /opt/tomcat/latest/conf/Catalina/localhost cd /opt/tomcat/latest/conf chown -R root:tomcat . chmod -R u+rwX,g+rX,o = . chmod -R g-w . cd /opt/tomcat/latest/webapps chown -R root:tomcat . chmod -R u+rwX,g+rX,o = . chmod -R g-w . cd /opt/tomcat/latest/logs chown -R tomcat:tomcat . chmod -R u+rwX,g+rX,o = . chmod -R g-w . cd /opt/tomcat/latest/temp chown -R tomcat:tomcat . chmod -R u+rwX,g+rX,o = . chmod -R g-w . cd /opt/tomcat/latest/work chown -R tomcat:tomcat . chmod -R u+rwX,g+rX,o = . chmod -R g-w . Remove example webapps Unless you have a good reason, these should all be removed. If you do need to keep these, you'll want to heavily restrict access to them. [root@login6devb ~]# cd /opt/tomcat/latest/ [root@login6devb latest]# rm -rf temp/* work/* [root@login6devb latest]# cd webapps/ [root@login6devb webapps]# rm -rf docs examples host-manager manager Note Important: The command above does not remove the ROOT web application from the webapps directory because it can be useful in a development/test environment to quickly determine whether Tomcat is working properly. However, when deploying Tomcat to production servers, the ROOT application should be removed along with the rest of the default web applications. To do Still need to add guidance on other aspects of the Tomcat Security Considerations document, such as server.xml configuration. References Tomcat Security Considerations","title":"Harden the installation"},{"location":"setting-up-the-environment/tomcat/tomcat-harden/#harden-the-installation","text":"Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. The Tomcat Security Considerations document makes several recommendations for hardening a Tomcat installation: Tomcat should not be run as the root user; it should be run as a dedicated user (usually named tomcat) that has minimum operating system permissions. It should not be possible to log in remotely as the tomcat user. All Tomcat files should be owned by user root and group tomcat (the tomcat user\u2019s default group should be group tomcat). File/directory permissions should be set to owner read/write, group read only, and world none. The exceptions are the logs, temp, and work directories, which should be owned by the tomcat user instead of root. The default and example web applications included with the Tomcat distribution should be removed if they are not needed. Auto-deployment should be disabled, and web applications should be deployed as exploded directories rather than web application archives (WAR files). Implementing these recommendations means that, even if an attacker compromises the Tomcat process, he or she cannot change the Tomcat configuration, deploy new web applications, or modify existing web applications.","title":"Harden the installation"},{"location":"setting-up-the-environment/tomcat/tomcat-harden/#create-a-tomcat-user-and-group","text":"[root@login6devb ~]# groupadd -r tomcat [root@login6devb ~]# useradd -r -d /opt/tomcat -g tomcat -s /sbin/nologin tomcat (This gives a 'shell' of /sbin/nologin which prevents the user from logging in)","title":"Create a tomcat user and group"},{"location":"setting-up-the-environment/tomcat/tomcat-harden/#set-file-ownership-and-permissions","text":"Some of these should be owned by user:root and group:tomcat (conf and webapps). Others (logs temp work) should be owned by user/group:tomcat. mkdir -p /opt/tomcat/latest/conf/Catalina/localhost cd /opt/tomcat/latest/conf chown -R root:tomcat . chmod -R u+rwX,g+rX,o = . chmod -R g-w . cd /opt/tomcat/latest/webapps chown -R root:tomcat . chmod -R u+rwX,g+rX,o = . chmod -R g-w . cd /opt/tomcat/latest/logs chown -R tomcat:tomcat . chmod -R u+rwX,g+rX,o = . chmod -R g-w . cd /opt/tomcat/latest/temp chown -R tomcat:tomcat . chmod -R u+rwX,g+rX,o = . chmod -R g-w . cd /opt/tomcat/latest/work chown -R tomcat:tomcat . chmod -R u+rwX,g+rX,o = . chmod -R g-w .","title":"Set file ownership and permissions"},{"location":"setting-up-the-environment/tomcat/tomcat-harden/#remove-example-webapps","text":"Unless you have a good reason, these should all be removed. If you do need to keep these, you'll want to heavily restrict access to them. [root@login6devb ~]# cd /opt/tomcat/latest/ [root@login6devb latest]# rm -rf temp/* work/* [root@login6devb latest]# cd webapps/ [root@login6devb webapps]# rm -rf docs examples host-manager manager Note Important: The command above does not remove the ROOT web application from the webapps directory because it can be useful in a development/test environment to quickly determine whether Tomcat is working properly. However, when deploying Tomcat to production servers, the ROOT application should be removed along with the rest of the default web applications.","title":"Remove example webapps"},{"location":"setting-up-the-environment/tomcat/tomcat-harden/#to-do","text":"Still need to add guidance on other aspects of the Tomcat Security Considerations document, such as server.xml configuration.","title":"To do"},{"location":"setting-up-the-environment/tomcat/tomcat-harden/#references","text":"Tomcat Security Considerations","title":"References"},{"location":"setting-up-the-environment/tomcat/tomcat-install/","text":"Tomcat Installation Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. As mentioned in the [architecture] section, RHEL 8 doesn't package Tomcat (and RHEL 7 only packaged Tomcat 8.0.x) so this is both how I've had Tomcat managed in RHEL 7 for CAS 5, as well as how I'm installing and maintaining Tomcat 9.0.x on RHEL 8. Note Your URL in the wget command below will differ depending on which mirror you are getting from the Tomcat Download page, and which specific version of Tomcat you are downloading (9.0.43 was the latest in the 9.0.x branch when this page was last updated.) [root@login6devb ~]# mkdir -p /opt/tomcat [root@login6devb ~]# cd /opt/tomcat [root@login6devb tomcat]# wget https://mirrors.ocf.berkeley.edu/apache/tomcat/tomcat-9/v9.0.43/bin/apache-tomcat-9.0.43.tar.gz (download progress will show) [root@login6devb tomcat]# tar xzf apache-tomcat-9.0.43.tar.gz [root@login6devb tomcat]# ln -s /opt/tomcat/apache-tomcat-9.0.43 latest [root@login6devb tomcat]# rm -f apache-tomcat-9.0.43.tar.gz [root@login6devb tomcat]#","title":"Install Tomcat"},{"location":"setting-up-the-environment/tomcat/tomcat-install/#tomcat-installation","text":"Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. As mentioned in the [architecture] section, RHEL 8 doesn't package Tomcat (and RHEL 7 only packaged Tomcat 8.0.x) so this is both how I've had Tomcat managed in RHEL 7 for CAS 5, as well as how I'm installing and maintaining Tomcat 9.0.x on RHEL 8. Note Your URL in the wget command below will differ depending on which mirror you are getting from the Tomcat Download page, and which specific version of Tomcat you are downloading (9.0.43 was the latest in the 9.0.x branch when this page was last updated.) [root@login6devb ~]# mkdir -p /opt/tomcat [root@login6devb ~]# cd /opt/tomcat [root@login6devb tomcat]# wget https://mirrors.ocf.berkeley.edu/apache/tomcat/tomcat-9/v9.0.43/bin/apache-tomcat-9.0.43.tar.gz (download progress will show) [root@login6devb tomcat]# tar xzf apache-tomcat-9.0.43.tar.gz [root@login6devb tomcat]# ln -s /opt/tomcat/apache-tomcat-9.0.43 latest [root@login6devb tomcat]# rm -f apache-tomcat-9.0.43.tar.gz [root@login6devb tomcat]#","title":"Tomcat Installation"},{"location":"setting-up-the-environment/tomcat/tomcat-organize/","text":"Organize the installation Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. To make it easier to upgrade (and if needed, downgrade) Tomcat without having to reapply configuration file changes, or reinstall web applications, it's better (in my opinion) to pull certain directories outside of Tomcat's own directory, them symlink them from the Tomcat directory to a more permanent place on the operating system. You should still review release notes for upgrades, in case there are any configuration changes which could cause issues. Move the configuration (conf) directory to /etc/tomcat The conf directory has Tomcat's configuration files. If you're like me - you prefer to manage your configuration for applications from /etc. We'll be initially moving the conf directory to /etc/tomcat to give it a starting point, but then will create a symlink to point /opt/tomcat/latest/conf to /etc/tomcat [root@login6devb ~]# cd /opt/tomcat/latest/ [root@login6devb latest]# cp -pR conf /etc/tomcat [root@login6devb latest]# rm -rf conf [root@login6devb latest]# ln -s /etc/tomcat conf Move the logs directory to /var/log/tomcat We'll be doing the same with the logs directory as with configuration. This way your log files persist across versions (and you don't have to go hunt down the version that was running when you need logs from day 'x'). You may also keep /var/log or /var as separate partitions to avoid log files filling up the root filesystem. [root@login6devb ~]# cd /opt/tomcat/latest/ [root@login6devb latest]# cp -pR logs /var/log/tomcat [root@login6devb latest]# rm -rf logs [root@login6devb latest]# ln -s /var/log/tomcat logs Move the webapps directory to /var/lib/tomcat Same drill as the last two - best to keep the webapps directory in a more permanent location. This way you don't have to move /opt/tomcat/apache-tomcat-{versionx}/webapps/cas.war to /opt/tomcat/apache-tomcat-{versiony}/webapps/ after each upgrade of Tomcat. [root@login6devb ~]# cd /opt/tomcat/latest/ [root@login6devb latest]# cp -pR webapps /var/lib/tomcat [root@login6devb latest]# rm -rf webapps [root@login6devb latest]# ln -s /var/lib/tomcat webapps Move the work directory to /var/cache/tomcat/work Tomcat\u2019s work directory is where translated servlet source files and JSP/JSF classes are stored. Its contents are created automatically, but don\u2019t need to be recreated unless the application has been changed. To reduce startup time, the contents of this directory should be preserved across application restarts and system reboots. Linux systems provide the /var/cache directory for just that purpose, so we can put the work directory there. To do this: [root@login6devb ~]# cd /opt/tomcat/latest/ [root@login6devb latest]# mkdir /var/cache/tomcat [root@login6devb latest]# cp -pR work /var/cache/tomcat/work [root@login6devb latest]# rm -rf work [root@login6devb latest]# ln -s /var/cache/tomcat/work work Move the temp directory to /var/cache/tomcat/temp Tomcat provides a temp directory for web applications to store temporary files in. But like log files, temporary files can sometimes be very large, so storing them in /opt is probably not a good practice. But /tmp and /var/tmp are not the best places either, because we want to be able to limit access to Tomcat\u2019s temporary files (see Harden the installation). Therefore, we will create a new temp directory under /var/cache/tomcat. To do this: [root@login6devb ~]# cd /opt/tomcat/latest/ [root@login6devb latest]# cp -pR temp /var/cache/tomcat/temp [root@login6devb latest]# rm -rf temp [root@login6devb latest]# ln -s /var/cache/tomcat/temp temp Final layout Your layout should look like the following:","title":"Organize the installation"},{"location":"setting-up-the-environment/tomcat/tomcat-organize/#organize-the-installation","text":"Tip The steps below are for the CAS servers - not the build server. That being said - if you are going to install and maintain via Ansible - you don't need to do this manually, though it is worth seeing how to do it manually. To make it easier to upgrade (and if needed, downgrade) Tomcat without having to reapply configuration file changes, or reinstall web applications, it's better (in my opinion) to pull certain directories outside of Tomcat's own directory, them symlink them from the Tomcat directory to a more permanent place on the operating system. You should still review release notes for upgrades, in case there are any configuration changes which could cause issues.","title":"Organize the installation"},{"location":"setting-up-the-environment/tomcat/tomcat-organize/#move-the-configuration-conf-directory-to-etctomcat","text":"The conf directory has Tomcat's configuration files. If you're like me - you prefer to manage your configuration for applications from /etc. We'll be initially moving the conf directory to /etc/tomcat to give it a starting point, but then will create a symlink to point /opt/tomcat/latest/conf to /etc/tomcat [root@login6devb ~]# cd /opt/tomcat/latest/ [root@login6devb latest]# cp -pR conf /etc/tomcat [root@login6devb latest]# rm -rf conf [root@login6devb latest]# ln -s /etc/tomcat conf","title":"Move the configuration (conf) directory to /etc/tomcat"},{"location":"setting-up-the-environment/tomcat/tomcat-organize/#move-the-logs-directory-to-varlogtomcat","text":"We'll be doing the same with the logs directory as with configuration. This way your log files persist across versions (and you don't have to go hunt down the version that was running when you need logs from day 'x'). You may also keep /var/log or /var as separate partitions to avoid log files filling up the root filesystem. [root@login6devb ~]# cd /opt/tomcat/latest/ [root@login6devb latest]# cp -pR logs /var/log/tomcat [root@login6devb latest]# rm -rf logs [root@login6devb latest]# ln -s /var/log/tomcat logs","title":"Move the logs directory to /var/log/tomcat"},{"location":"setting-up-the-environment/tomcat/tomcat-organize/#move-the-webapps-directory-to-varlibtomcat","text":"Same drill as the last two - best to keep the webapps directory in a more permanent location. This way you don't have to move /opt/tomcat/apache-tomcat-{versionx}/webapps/cas.war to /opt/tomcat/apache-tomcat-{versiony}/webapps/ after each upgrade of Tomcat. [root@login6devb ~]# cd /opt/tomcat/latest/ [root@login6devb latest]# cp -pR webapps /var/lib/tomcat [root@login6devb latest]# rm -rf webapps [root@login6devb latest]# ln -s /var/lib/tomcat webapps","title":"Move the webapps directory to /var/lib/tomcat"},{"location":"setting-up-the-environment/tomcat/tomcat-organize/#move-the-work-directory-to-varcachetomcatwork","text":"Tomcat\u2019s work directory is where translated servlet source files and JSP/JSF classes are stored. Its contents are created automatically, but don\u2019t need to be recreated unless the application has been changed. To reduce startup time, the contents of this directory should be preserved across application restarts and system reboots. Linux systems provide the /var/cache directory for just that purpose, so we can put the work directory there. To do this: [root@login6devb ~]# cd /opt/tomcat/latest/ [root@login6devb latest]# mkdir /var/cache/tomcat [root@login6devb latest]# cp -pR work /var/cache/tomcat/work [root@login6devb latest]# rm -rf work [root@login6devb latest]# ln -s /var/cache/tomcat/work work","title":"Move the work directory to /var/cache/tomcat/work"},{"location":"setting-up-the-environment/tomcat/tomcat-organize/#move-the-temp-directory-to-varcachetomcattemp","text":"Tomcat provides a temp directory for web applications to store temporary files in. But like log files, temporary files can sometimes be very large, so storing them in /opt is probably not a good practice. But /tmp and /var/tmp are not the best places either, because we want to be able to limit access to Tomcat\u2019s temporary files (see Harden the installation). Therefore, we will create a new temp directory under /var/cache/tomcat. To do this: [root@login6devb ~]# cd /opt/tomcat/latest/ [root@login6devb latest]# cp -pR temp /var/cache/tomcat/temp [root@login6devb latest]# rm -rf temp [root@login6devb latest]# ln -s /var/cache/tomcat/temp temp","title":"Move the temp directory to /var/cache/tomcat/temp"},{"location":"setting-up-the-environment/tomcat/tomcat-organize/#final-layout","text":"Your layout should look like the following:","title":"Final layout"},{"location":"setting-up-the-environment/tomcat/tomcat-overview/","text":"Tomcat overview Summary Apache Tomcat will be used as the Java Servlet container for CAS. To ensure best practices for security, and performance, the latest versions of OpenSSL, the Tomcat Native Library, the Apache Portable Runtime, and the latest in the 9.0.x series of Apache Tomcat will be used. Tomcat architecture For our Tomcat installation, I don't like to rely on the Red Hat distributed versions. This is since with RHEL 7 - they only provided Tomcat 8.0.x from their repositories, and with RHEL 8 - it's only provided as part of JBoss. Using this method - you get an easy to maintain environment, which relies on symlinks to keep the 'important' stuff (configurations, log files, servlets) outside of the installation directories. When done - you will have /opt/tomcat - and within that you'll have a directory for each Tomcat install you have (for example, /opt/tomcat/apache-tomcat-9.0.39 or /opt/tomcat/apache-tomcat-9.0.43), and a symlink (/opt/tomcat/latest) which points to the in-use version. This allows for a quick upgrade and rollback if there are issues. Then - within /opt/tomcat/apache-tomcat-9.0.x, you'll have symlinks to where things actually live, for example: /opt/tomcat/apache-tomcat-9.0.39/conf is a symlink to /etc/tomcat /opt/tomcat/apache-tomcat-9.0.39/logs is a symlink to /var/log/tomcat /opt/tomcat/apache-tomcat-9.0.39/temp is a symlink to /var/cache/tomcat/temp /opt/tomcat/apache-tomcat-9.0.39/webapps is a symlink to /var/lib/tomcat /opt/tomcat/apache-tomcat-9.0.39/work is a symlink to /var/cache/tomcat/work Components: EPEL (Extra Packages for Enterprise Linux) - which is needed for a couple packages. Haveged - for better entropy in random number generation than what is included by default Apache Portable Runtime Tomcat Native Library Apache Commons Daemon OpenJDK Apache HTTPD - since I hate doing SSL/TLS and a number of other things in Apache Tomcat directly. Prerequisites gcc make dnf install gcc make","title":"Tomcat Overview"},{"location":"setting-up-the-environment/tomcat/tomcat-overview/#tomcat-overview","text":"Summary Apache Tomcat will be used as the Java Servlet container for CAS. To ensure best practices for security, and performance, the latest versions of OpenSSL, the Tomcat Native Library, the Apache Portable Runtime, and the latest in the 9.0.x series of Apache Tomcat will be used.","title":"Tomcat overview"},{"location":"setting-up-the-environment/tomcat/tomcat-overview/#tomcat-architecture","text":"For our Tomcat installation, I don't like to rely on the Red Hat distributed versions. This is since with RHEL 7 - they only provided Tomcat 8.0.x from their repositories, and with RHEL 8 - it's only provided as part of JBoss. Using this method - you get an easy to maintain environment, which relies on symlinks to keep the 'important' stuff (configurations, log files, servlets) outside of the installation directories. When done - you will have /opt/tomcat - and within that you'll have a directory for each Tomcat install you have (for example, /opt/tomcat/apache-tomcat-9.0.39 or /opt/tomcat/apache-tomcat-9.0.43), and a symlink (/opt/tomcat/latest) which points to the in-use version. This allows for a quick upgrade and rollback if there are issues. Then - within /opt/tomcat/apache-tomcat-9.0.x, you'll have symlinks to where things actually live, for example: /opt/tomcat/apache-tomcat-9.0.39/conf is a symlink to /etc/tomcat /opt/tomcat/apache-tomcat-9.0.39/logs is a symlink to /var/log/tomcat /opt/tomcat/apache-tomcat-9.0.39/temp is a symlink to /var/cache/tomcat/temp /opt/tomcat/apache-tomcat-9.0.39/webapps is a symlink to /var/lib/tomcat /opt/tomcat/apache-tomcat-9.0.39/work is a symlink to /var/cache/tomcat/work","title":"Tomcat architecture"},{"location":"setting-up-the-environment/tomcat/tomcat-overview/#components","text":"EPEL (Extra Packages for Enterprise Linux) - which is needed for a couple packages. Haveged - for better entropy in random number generation than what is included by default Apache Portable Runtime Tomcat Native Library Apache Commons Daemon OpenJDK Apache HTTPD - since I hate doing SSL/TLS and a number of other things in Apache Tomcat directly.","title":"Components:"},{"location":"setting-up-the-environment/tomcat/tomcat-overview/#prerequisites","text":"gcc make dnf install gcc make","title":"Prerequisites"},{"location":"setting-up-the-environment/tomcat-ansible/handlers/","text":"Ansible Handlers setup for Tomcat Handlers in Ansible are used to run operations based on a change. For example, a template command in a playbook could have a 'notify: restart apache' section at the end. This would be run (at the end of the playbook) if that template command changes something. If nothing changes, with that specific template command at least, then that notify command wouldn't be triggered. # handlers file for apache-tomcat # All that's really here for now is just commands to start, stop, and restart tomcat. - name : stop tomcat ansible.builtin.systemd : name : tomcat state : stopped - name : start tomcat ansible.builtin.systemd : name : tomcat state : started - name : restart tomcat ansible.builtin.systemd : name : tomcat state : restarted References Ansible systemd module Ansible - Handlers: running operations on change","title":"Setup Handlers"},{"location":"setting-up-the-environment/tomcat-ansible/handlers/#ansible-handlers-setup-for-tomcat","text":"Handlers in Ansible are used to run operations based on a change. For example, a template command in a playbook could have a 'notify: restart apache' section at the end. This would be run (at the end of the playbook) if that template command changes something. If nothing changes, with that specific template command at least, then that notify command wouldn't be triggered. # handlers file for apache-tomcat # All that's really here for now is just commands to start, stop, and restart tomcat. - name : stop tomcat ansible.builtin.systemd : name : tomcat state : stopped - name : start tomcat ansible.builtin.systemd : name : tomcat state : started - name : restart tomcat ansible.builtin.systemd : name : tomcat state : restarted","title":"Ansible Handlers setup for Tomcat"},{"location":"setting-up-the-environment/tomcat-ansible/handlers/#references","text":"Ansible systemd module Ansible - Handlers: running operations on change","title":"References"},{"location":"setting-up-the-environment/tomcat-ansible/running-the-play/","text":"Running the play to install or upgrade Tomcat Before running - check that you have the following ready: The variables within the role's vars directory (see Setup Templates ) The templates within the role's templates directory (see Setup Templates ) The tasks directory including main.yml and all sub-tasks that it calls in the role's tasks directory (see Creating Tasks ) The ssh public key from the .ssh directory of the user on your ansible host, has been copied to the authorized_keys file of your target host (see Setup SSH public key authentication ) Running the playbook Once you have your Tomcat role ready (see the remaining pages) out - you would run it as follows: ansible-playbook site.yml --limit <yourCasServer> # Example - assuming CAS6_DEV is a grouping in site.yml of your CAS dev environment: ansible-playbook site.yml --limit CAS6_DEV # Or just one server: ansible-playbook site.yml --limit login6devb You may get warnings or errors. You will have to review those.","title":"Running the play"},{"location":"setting-up-the-environment/tomcat-ansible/running-the-play/#running-the-play-to-install-or-upgrade-tomcat","text":"","title":"Running the play to install or upgrade Tomcat"},{"location":"setting-up-the-environment/tomcat-ansible/running-the-play/#before-running-check-that-you-have-the-following-ready","text":"The variables within the role's vars directory (see Setup Templates ) The templates within the role's templates directory (see Setup Templates ) The tasks directory including main.yml and all sub-tasks that it calls in the role's tasks directory (see Creating Tasks ) The ssh public key from the .ssh directory of the user on your ansible host, has been copied to the authorized_keys file of your target host (see Setup SSH public key authentication )","title":"Before running - check that you have the following ready:"},{"location":"setting-up-the-environment/tomcat-ansible/running-the-play/#running-the-playbook","text":"Once you have your Tomcat role ready (see the remaining pages) out - you would run it as follows: ansible-playbook site.yml --limit <yourCasServer> # Example - assuming CAS6_DEV is a grouping in site.yml of your CAS dev environment: ansible-playbook site.yml --limit CAS6_DEV # Or just one server: ansible-playbook site.yml --limit login6devb You may get warnings or errors. You will have to review those.","title":"Running the playbook"},{"location":"setting-up-the-environment/tomcat-ansible/tasks/","text":"Task setup Main task file To keep things cleaner, I include chunks of the role in separate files. All must be referenced by the main.yml file within the role (ansible/roles/apache-tomcat/tasks/main.yml). They are called in order by main.yml: # tasks file for apache-tomcat - include_tasks : setup-prerequisites.yml # on RHEL7 - I have a playbook to download/compile OpenSSL as well but that's not needed on RHEL8. RHEL7 had a really old version of OpenSSL. - include_tasks : setup-apr.yml - include_tasks : install.yml - include_tasks : setup-tomcat-native.yml - include_tasks : setup-commons-daemon.yml - include_tasks : configure-tomcat.yml - include_tasks : post-install.yml Tip If you haven't used ansible - you may want to look up idempotency and what it means. An operation is idempotent if the result of performing it once is exactly the same as the result of performing it repeatedly without any intervening actions. For example in these playbooks, there are tasks to create users, create directories, or apply a config template. If Ansible detects that these are already right - it will just mosey right along and not change anything for those steps. Setup Prerequisites I've left some of my older stuff in here for example's sake (since again, I still have to maintain some RHEL 7 and JDK 8 systems). I've highlighted the revelant portions for this. The example isn't the cleanest, but you can see how you can alter packages based on different needs (in thise case, dnf is used for RHEL 8 and yum for RHEL 7.). The Tomcat user and group are created as well at the bottom. If you use this, the 'jdk_version\" needs to be specified in your ansible hosts file, or some variable file. The ansible_distribution and ansible_distribution_major_version don't need to be specified as Ansible reads those from the hosts it connects to. - name : Setup prerequisite dnf packages (RHEL 8 & JDK 11) dnf : name : - epel-release - java-11-openjdk - java-11-openjdk-devel - haveged - gcc - libtool - make - openssl-devel state : present when : ansible_distribution == 'RedHat' and ansible_distribution_major_version == '8' and jdk_version == 11 - name : Setup prerequisite yum packages (RHEL 7 & JDK 11) yum : name : - epel-release - java-11-openjdk - java-11-openjdk-devel - haveged - gcc - libtool - make - policycoreutils-python state : present when : ansible_distribution == 'RedHat' and ansible_distribution_major_version == '7' and jdk_version == 11 - name : Setup prerequisite yum packages (RHEL 7 & JDK 8) yum : name : - epel-release - java-1.8.0-openjdk - java-1.8.0-openjdk-devel - haveged - gcc - libtool - make - policycoreutils-python state : present when : ansible_distribution == 'RedHat' and ansible_distribution_major_version == '7' and jdk_version == 8 - name : Add tomcat group group : name : tomcat - name : Add tomcat user user : name : tomcat group : tomcat home : /opt/tomcat shell : /sbin/nologin createhome : yes system : yes Setup Apache Portable Runtime (APR) The following section checks if the target version of APR is already installed. If not, it will download, unpack, compile, and install it. - name : Check if APR {{ apr_ver }} is already installed stat : path : /opt/apr/apr-{{ apr_ver }}/bin/apr-1-config register : apr_binary - name : Create base APR directory if it doesn't exist file : path : /opt/apr state : directory owner : root group : root - name : Create APR {{ apr_ver }} directory if it doesn't exist file : path : /opt/apr/apr-{{ apr_ver }} state : directory owner : root group : root - name : Check if APR {{ apr_ver }} is already downloaded stat : path : /root/apr-{{ apr_ver }}.tar.gz register : apr_tarball # Only download APR if APR {{ apr_ver }} isn't already installed, and the tarball isn't downloaded - name : Download APR get_url : url : \"{{ apr_archive_url }}\" dest : \"{{ apr_archive_dest }}\" when : apr_tarball.stat.exists == False and apr_binary.stat.exists == False # Only unpack apr archive if the apr binary doesn't already exist - name : Unpack APR archive unarchive : src : \"{{ apr_archive_dest }}\" dest : /root/ owner : root group : root remote_src : yes when : apr_binary.stat.exists == False # Only reconfigure if the APR directory for the version didn't already exist - name : Configure APR source command : ./configure --prefix=/opt/apr/apr-{{ apr_ver }} args : chdir : \"/root/apr-{{ apr_ver }}\" when : apr_binary.stat.exists == False # Only make if the APR directory for the version didn't already exist - name : Compile/install APR shell : make && make install args : chdir : \"/root/apr-{{ apr_ver }}\" when : apr_binary.stat.exists == False ## Handle APR symlink creation or repointing - name : Check if apr latest symlink exists stat : path : /opt/apr/latest register : apr_symlink # Create symlink if none exists, or repoint it if it is pointing to an older version - name : Create apr latest symlink to point to newly installed version file : src : \"/opt/apr/apr-{{ apr_ver }}\" dest : \"/opt/apr/latest\" owner : root group : root state : link when : apr_symlink.stat.exists == False or apr_symlink.stat.islnk # Cleanup - name : Remove apr source directory file : path : /root/apr-{{ apr_ver }} state : absent - name : Remove apr source tarball file : path : /root/apr-{{ apr_ver }}.tar.gz state : absent Install Apache Tomcat This section checks if the target version Tomcat is already installed. If not - it will download Tomcat, unpack the archive, and ensure the various directory symlinks (conf, log, temp, webapps, work) are already setup. If this is the first time Tomcat is being deployed on the target system (or the /etc/tomcat directory is otherwise empty), the contents of 'conf' from the downloaded tarball will be copied to /etc/tomcat. It will also setup the systemd unit file so it can eventually be started and set to start on boot. - name : Check if Tomcat {{ tomcat_ver }} directory exists stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }} register : tomcat_directory - name : Check if Tomcat {{ tomcat_ver }} tarball exists stat : path : \"{{ tomcat_archive_dest }}\" register : tomcat_tarball - name : Download Tomcat 8.5.x get_url : url : https://archive.apache.org/dist/tomcat/tomcat-8/v{{ tomcat_ver }}/bin/apache-tomcat-{{ tomcat_ver }}.tar.gz dest : \"{{ tomcat_archive_dest }}\" when : tomcat_tarball.stat.exists == False and tomcat_directory.stat.exists == False and tomcat_major_ver == 8.5 - name : Download Tomcat 9.0.x get_url : url : https://archive.apache.org/dist/tomcat/tomcat-9/v{{ tomcat_ver }}/bin/apache-tomcat-{{ tomcat_ver }}.tar.gz dest : \"{{ tomcat_archive_dest }}\" when : tomcat_tarball.stat.exists == False and tomcat_directory.stat.exists == False and tomcat_major_ver == 9.0 # Only unpack tomcat archive if the unpacked directory does not exist. # This cannot be used to install the same version without deletion of the old one - name : Unpack tomcat archive unarchive : src : \"{{ tomcat_archive_dest }}\" dest : /opt/tomcat owner : tomcat group : tomcat remote_src : yes when : tomcat_directory.stat.exists == False # Create the permanent tomcat conf, log, temp, webapps, work directories for later symlinking - name : Create Tomcat permanent conf directory file : path : /etc/tomcat state : directory owner : root group : tomcat mode : \"u=rwx,g=rx\" - name : Create Tomcat permanent log directory file : path : /var/log/tomcat state : directory owner : tomcat group : tomcat mode : \"u=rwx,g=rx\" - name : Create Tomcat permanent temp directory file : path : /var/cache/tomcat/temp state : directory owner : tomcat group : tomcat mode : \"u=rwx,g=rx\" - name : Create Tomcat permanent webapps directory file : path : /var/lib/tomcat state : directory owner : tomcat group : tomcat mode : \"u=rwx,g=rx\" - name : Create Tomcat permanent work directory file : path : /var/cache/tomcat/work state : directory owner : tomcat group : tomcat mode : \"u=rwx,g=rx\" # If server.xml exists - then we can assume /etc/tomcat has been populated. # - If so - don't change it # - If not, assume it hasn't been populated and copy the new Tomcat's /conf to /etc/tomcat/ - name : Check if server.xml exists stat : path : /etc/tomcat/server.xml register : server_xml - name : Copy files from conf directory copy : src : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/conf/ dest : /etc/tomcat/ remote_src : yes directory_mode : yes when : server_xml.stat.exists == False # If ROOT exists - then we can assume /var/lib/tomcat has been populated. # - If so - don't change it. # - If not, assume it hasn't been populated and copy the new Tomcat's ROOT over # - This needs to be updated to handle version updates of ROOT - name : Check ROOT exists stat : path : /var/lib/tomcat/ROOT register : root_webapps - name : Copy ROOT webapp directory copy : src : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/webapps/ROOT dest : /var/lib/tomcat/ remote_src : yes directory_mode : yes when : root_webapps.stat.exists == False ## remove directories (conf, logs, temp, webapps, work) and recreate as symlinks # conf - name : check {{ tomcat_ver }} conf directory stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/conf register : tomcat_conf_dir - name : Remove conf directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/conf state : absent when : tomcat_conf_dir.stat.isdir - name : Create conf symlink file : src : /etc/tomcat dest : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/conf owner : root group : tomcat state : link when : tomcat_conf_dir.stat.isdir or tomcat_conf_dir.stat.exists == False # logs - name : check {{ tomcat_ver }} logs directory stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/logs register : tomcat_logs_dir - name : Remove logs directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/logs state : absent when : tomcat_logs_dir.stat.isdir - name : Create logs symlink file : src : /var/log/tomcat dest : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/logs owner : tomcat group : tomcat state : link when : tomcat_logs_dir.stat.isdir or tomcat_logs_dir.stat.exists == False # temp - name : check {{ tomcat_ver }} temp directory stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/temp register : tomcat_temp_dir - name : Remove temp directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/temp state : absent when : tomcat_temp_dir.stat.isdir - name : Create temp symlink file : src : /var/cache/tomcat/temp dest : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/temp owner : tomcat group : tomcat state : link when : tomcat_temp_dir.stat.isdir or tomcat_temp_dir.stat.exists == False # webapps - name : check {{ tomcat_ver }} webapps directory stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/webapps register : tomcat_webapps_dir - name : Remove webapps directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/webapps state : absent when : tomcat_webapps_dir.stat.isdir - name : Create webapps symlink file : src : /var/lib/tomcat dest : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/webapps owner : root group : tomcat state : link when : tomcat_webapps_dir.stat.isdir or tomcat_webapps_dir.stat.exists == False # work - name : check {{ tomcat_ver }} work directory stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/work register : tomcat_work_dir - name : Remove webapps directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/work state : absent when : tomcat_work_dir.stat.isdir - name : Create work symlink file : src : /var/cache/tomcat/work dest : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/work owner : tomcat group : tomcat state : link when : tomcat_work_dir.stat.isdir or tomcat_webapps_dir.stat.exists == False # Setup systemd startup/shutdown script - name : Setup systemd startup/shutdown script template : src : tomcat.service.j2 dest : /etc/systemd/system/tomcat.service mode : 0644 owner : root group : root register : tomcat_service - name : Apply new SELinux file context to tomcat.service command : restorecon /etc/systemd/system/tomcat.service when : tomcat_service.changed - name : Reload systemd daemons after service update command : systemctl daemon-reload when : tomcat_service.changed Setup Tomcat Native Library This is where the Tomcat Native Library (if not already installed) gets unpacked, configured, compiled, and installed. - name : Check if Tomcat Native Library {{ tomcat_native_ver }} is already installed stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/lib/libtcnative-1.so register : tomcat_native_library # Untar tomcat native library - name : Unpack tomcat native library archive unarchive : src : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/tomcat-native.tar.gz\" dest : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/ owner : tomcat group : tomcat remote_src : yes when : tomcat_native_library.stat.exists == False # Only configure if the Tomcat Native Library directory for the version didn't already exist - name : Configure Tomcat Native Library (RHEL7 & JDK 11) command : \"./configure --with-java-home={{ JAVA_HOME }} --with-apr=/opt/apr/latest/bin/apr-1-config --prefix=/opt/tomcat/apache-tomcat-{{ tomcat_ver }}\" args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/tomcat-native-{{ tomcat_native_ver }}-src/native\" when : tomcat_native_library.stat.exists == False and ansible_distribution == 'RedHat' and ansible_distribution_major_version == '7' and jdk_version == 11 # Only configure if the Tomcat Native Library directory for the version didn't already exist - name : Configure Tomcat Native Library (RHEL7 & JDK 8) command : \"./configure --with-java-home={{ JAVA_8_HOME }} --with-apr=/opt/apr/latest/bin/apr-1-config --prefix=/opt/tomcat/apache-tomcat-{{ tomcat_ver }}\" args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/tomcat-native-{{ tomcat_native_ver }}-src/native\" when : tomcat_native_library.stat.exists == False and ansible_distribution == 'RedHat' and ansible_distribution_major_version == '7' and jdk_version == 8 # Only configure if the Tomcat Native Library directory for the version didn't already exist - name : Configure Tomcat Native Library (RHEL8) command : \"./configure --with-java-home={{ JAVA_HOME }} --with-apr=/opt/apr/latest/bin/apr-1-config --with-ssl=yes --prefix=/opt/tomcat/apache-tomcat-{{ tomcat_ver }}\" args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/tomcat-native-{{ tomcat_native_ver }}-src/native\" when : tomcat_native_library.stat.exists == False and ansible_distribution == 'RedHat' and ansible_distribution_major_version == '8' # Only make if the Tomcat Native Library directory for the version didn't already exist - name : Compile/install Tomcat Native Library shell : make && make install args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/tomcat-native-{{ tomcat_native_ver }}-src/native\" when : tomcat_native_library.stat.exists == False # Cleanup - name : Remove tomcat native source directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/tomcat-native-{{ tomcat_native_ver }}-src state : absent Setup Apache Tomcat Commons Daemon This is where the Tomcat Commons Daemon (if not already installed) gets unpacked, configured, compiled, and installed. - name : Check if Apache Tomcat Commons Daemon {{ commons_daemon_ver }} is already installed stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/jsvc register : commons_daemon_jsvc # Untar tomcat native library - name : Unpack Tomcat Commons Daemon native library unarchive : src : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/commons-daemon-native.tar.gz\" dest : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/\" owner : tomcat group : tomcat remote_src : yes when : commons_daemon_jsvc.stat.exists == False # Only configure if the Tomcat Native Library directory for the version didn't already exist - name : Configure Tomcat Commons Daemon native library (JDK 11) command : \"./configure --with-java={{ JAVA_HOME }}\" args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/commons-daemon-{{ commons_daemon_ver }}-native-src/unix\" when : commons_daemon_jsvc.stat.exists == False and jdk_version == 11 # Only configure if the Tomcat Native Library directory for the version didn't already exist - name : Configure Tomcat Commons Daemon native library (JDK 8) command : \"./configure --with-java={{ JAVA_8_HOME }}\" args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/commons-daemon-{{ commons_daemon_ver }}-native-src/unix\" when : commons_daemon_jsvc.stat.exists == False and jdk_version == 8 # Only make if the Tomcat Native Library directory for the version didn't already exist - name : Compile Tomcat Commons Daemon native library shell : make args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/commons-daemon-{{ commons_daemon_ver }}-native-src/unix\" when : commons_daemon_jsvc.stat.exists == False - name : Move jsvc file copy : src : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/commons-daemon-{{ commons_daemon_ver }}-native-src/unix/jsvc\" dest : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/\" remote_src : yes owner : root group : tomcat mode : 0755 when : commons_daemon_jsvc.stat.exists == False # Cleanup - name : Remove commons-daemon source directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/commons-daemon-{{ commons_daemon_ver }}-native-src state : absent Configure Tomcat This is where we take the template files that were created earlier for web.xml, server.xml, etc., and make sure they are copied over to the server. If any are updated, it will notify Tomcat to restart when done. This is basically just configuring the various files in /etc/tomcat. To initially create these files - download copies of them from a default Tomcat install to your Ansible host's roles/apache-tomcat/templates directory and alter them as needed. I will have copies of these files uploaded as well. To do - add links to the documents where these file changes are done. - name : Setup catalina.properties template : src : cas6-catalina.properties.j2 dest : /etc/tomcat/catalina.properties mode : 0640 owner : root group : tomcat when : (\"login\" in inventory_hostname) notify : restart tomcat - name : Setup context.xml template : src : cas6-context.xml.j2 dest : /etc/tomcat/context.xml mode : 0640 owner : root group : tomcat when : (\"login\" in inventory_hostname) notify : restart tomcat - name : Setup server.xml template : src : cas6-server.xml.j2 dest : /etc/tomcat/server.xml mode : 0640 owner : root group : tomcat when : (\"login\" in inventory_hostname) notify : restart tomcat - name : Setup web.xml template : src : cas6-web.xml.j2 dest : /etc/tomcat/web.xml mode : 0640 owner : root group : tomcat when : (\"login\" in inventory_hostname) notify : restart tomcat Post installation tasks If the installation fails (errors, not warnings) at any point - it won't get to here, so it shouldn't move the symlink and restart tomcat unless the install completed. It also won't 'notify' tomcat to restart if they didn't even have to create the symlink (or if the config files changed earlier) - name : Setup Apache Tomcat {{ tomcat_ver }} symlink file : src : /opt/tomcat/apache-tomcat-{{ tomcat_ver }} dest : /opt/tomcat/latest owner : root group : root state : link notify : restart tomcat # Cleanup - name : Remove tomcat tarball file : path : /root/apache-tomcat-{{ tomcat_ver }}.tar.gz state : absent","title":"Creating tasks"},{"location":"setting-up-the-environment/tomcat-ansible/tasks/#task-setup","text":"","title":"Task setup"},{"location":"setting-up-the-environment/tomcat-ansible/tasks/#main-task-file","text":"To keep things cleaner, I include chunks of the role in separate files. All must be referenced by the main.yml file within the role (ansible/roles/apache-tomcat/tasks/main.yml). They are called in order by main.yml: # tasks file for apache-tomcat - include_tasks : setup-prerequisites.yml # on RHEL7 - I have a playbook to download/compile OpenSSL as well but that's not needed on RHEL8. RHEL7 had a really old version of OpenSSL. - include_tasks : setup-apr.yml - include_tasks : install.yml - include_tasks : setup-tomcat-native.yml - include_tasks : setup-commons-daemon.yml - include_tasks : configure-tomcat.yml - include_tasks : post-install.yml Tip If you haven't used ansible - you may want to look up idempotency and what it means. An operation is idempotent if the result of performing it once is exactly the same as the result of performing it repeatedly without any intervening actions. For example in these playbooks, there are tasks to create users, create directories, or apply a config template. If Ansible detects that these are already right - it will just mosey right along and not change anything for those steps.","title":"Main task file"},{"location":"setting-up-the-environment/tomcat-ansible/tasks/#setup-prerequisites","text":"I've left some of my older stuff in here for example's sake (since again, I still have to maintain some RHEL 7 and JDK 8 systems). I've highlighted the revelant portions for this. The example isn't the cleanest, but you can see how you can alter packages based on different needs (in thise case, dnf is used for RHEL 8 and yum for RHEL 7.). The Tomcat user and group are created as well at the bottom. If you use this, the 'jdk_version\" needs to be specified in your ansible hosts file, or some variable file. The ansible_distribution and ansible_distribution_major_version don't need to be specified as Ansible reads those from the hosts it connects to. - name : Setup prerequisite dnf packages (RHEL 8 & JDK 11) dnf : name : - epel-release - java-11-openjdk - java-11-openjdk-devel - haveged - gcc - libtool - make - openssl-devel state : present when : ansible_distribution == 'RedHat' and ansible_distribution_major_version == '8' and jdk_version == 11 - name : Setup prerequisite yum packages (RHEL 7 & JDK 11) yum : name : - epel-release - java-11-openjdk - java-11-openjdk-devel - haveged - gcc - libtool - make - policycoreutils-python state : present when : ansible_distribution == 'RedHat' and ansible_distribution_major_version == '7' and jdk_version == 11 - name : Setup prerequisite yum packages (RHEL 7 & JDK 8) yum : name : - epel-release - java-1.8.0-openjdk - java-1.8.0-openjdk-devel - haveged - gcc - libtool - make - policycoreutils-python state : present when : ansible_distribution == 'RedHat' and ansible_distribution_major_version == '7' and jdk_version == 8 - name : Add tomcat group group : name : tomcat - name : Add tomcat user user : name : tomcat group : tomcat home : /opt/tomcat shell : /sbin/nologin createhome : yes system : yes","title":"Setup Prerequisites"},{"location":"setting-up-the-environment/tomcat-ansible/tasks/#setup-apache-portable-runtime-apr","text":"The following section checks if the target version of APR is already installed. If not, it will download, unpack, compile, and install it. - name : Check if APR {{ apr_ver }} is already installed stat : path : /opt/apr/apr-{{ apr_ver }}/bin/apr-1-config register : apr_binary - name : Create base APR directory if it doesn't exist file : path : /opt/apr state : directory owner : root group : root - name : Create APR {{ apr_ver }} directory if it doesn't exist file : path : /opt/apr/apr-{{ apr_ver }} state : directory owner : root group : root - name : Check if APR {{ apr_ver }} is already downloaded stat : path : /root/apr-{{ apr_ver }}.tar.gz register : apr_tarball # Only download APR if APR {{ apr_ver }} isn't already installed, and the tarball isn't downloaded - name : Download APR get_url : url : \"{{ apr_archive_url }}\" dest : \"{{ apr_archive_dest }}\" when : apr_tarball.stat.exists == False and apr_binary.stat.exists == False # Only unpack apr archive if the apr binary doesn't already exist - name : Unpack APR archive unarchive : src : \"{{ apr_archive_dest }}\" dest : /root/ owner : root group : root remote_src : yes when : apr_binary.stat.exists == False # Only reconfigure if the APR directory for the version didn't already exist - name : Configure APR source command : ./configure --prefix=/opt/apr/apr-{{ apr_ver }} args : chdir : \"/root/apr-{{ apr_ver }}\" when : apr_binary.stat.exists == False # Only make if the APR directory for the version didn't already exist - name : Compile/install APR shell : make && make install args : chdir : \"/root/apr-{{ apr_ver }}\" when : apr_binary.stat.exists == False ## Handle APR symlink creation or repointing - name : Check if apr latest symlink exists stat : path : /opt/apr/latest register : apr_symlink # Create symlink if none exists, or repoint it if it is pointing to an older version - name : Create apr latest symlink to point to newly installed version file : src : \"/opt/apr/apr-{{ apr_ver }}\" dest : \"/opt/apr/latest\" owner : root group : root state : link when : apr_symlink.stat.exists == False or apr_symlink.stat.islnk # Cleanup - name : Remove apr source directory file : path : /root/apr-{{ apr_ver }} state : absent - name : Remove apr source tarball file : path : /root/apr-{{ apr_ver }}.tar.gz state : absent","title":"Setup Apache Portable Runtime (APR)"},{"location":"setting-up-the-environment/tomcat-ansible/tasks/#install-apache-tomcat","text":"This section checks if the target version Tomcat is already installed. If not - it will download Tomcat, unpack the archive, and ensure the various directory symlinks (conf, log, temp, webapps, work) are already setup. If this is the first time Tomcat is being deployed on the target system (or the /etc/tomcat directory is otherwise empty), the contents of 'conf' from the downloaded tarball will be copied to /etc/tomcat. It will also setup the systemd unit file so it can eventually be started and set to start on boot. - name : Check if Tomcat {{ tomcat_ver }} directory exists stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }} register : tomcat_directory - name : Check if Tomcat {{ tomcat_ver }} tarball exists stat : path : \"{{ tomcat_archive_dest }}\" register : tomcat_tarball - name : Download Tomcat 8.5.x get_url : url : https://archive.apache.org/dist/tomcat/tomcat-8/v{{ tomcat_ver }}/bin/apache-tomcat-{{ tomcat_ver }}.tar.gz dest : \"{{ tomcat_archive_dest }}\" when : tomcat_tarball.stat.exists == False and tomcat_directory.stat.exists == False and tomcat_major_ver == 8.5 - name : Download Tomcat 9.0.x get_url : url : https://archive.apache.org/dist/tomcat/tomcat-9/v{{ tomcat_ver }}/bin/apache-tomcat-{{ tomcat_ver }}.tar.gz dest : \"{{ tomcat_archive_dest }}\" when : tomcat_tarball.stat.exists == False and tomcat_directory.stat.exists == False and tomcat_major_ver == 9.0 # Only unpack tomcat archive if the unpacked directory does not exist. # This cannot be used to install the same version without deletion of the old one - name : Unpack tomcat archive unarchive : src : \"{{ tomcat_archive_dest }}\" dest : /opt/tomcat owner : tomcat group : tomcat remote_src : yes when : tomcat_directory.stat.exists == False # Create the permanent tomcat conf, log, temp, webapps, work directories for later symlinking - name : Create Tomcat permanent conf directory file : path : /etc/tomcat state : directory owner : root group : tomcat mode : \"u=rwx,g=rx\" - name : Create Tomcat permanent log directory file : path : /var/log/tomcat state : directory owner : tomcat group : tomcat mode : \"u=rwx,g=rx\" - name : Create Tomcat permanent temp directory file : path : /var/cache/tomcat/temp state : directory owner : tomcat group : tomcat mode : \"u=rwx,g=rx\" - name : Create Tomcat permanent webapps directory file : path : /var/lib/tomcat state : directory owner : tomcat group : tomcat mode : \"u=rwx,g=rx\" - name : Create Tomcat permanent work directory file : path : /var/cache/tomcat/work state : directory owner : tomcat group : tomcat mode : \"u=rwx,g=rx\" # If server.xml exists - then we can assume /etc/tomcat has been populated. # - If so - don't change it # - If not, assume it hasn't been populated and copy the new Tomcat's /conf to /etc/tomcat/ - name : Check if server.xml exists stat : path : /etc/tomcat/server.xml register : server_xml - name : Copy files from conf directory copy : src : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/conf/ dest : /etc/tomcat/ remote_src : yes directory_mode : yes when : server_xml.stat.exists == False # If ROOT exists - then we can assume /var/lib/tomcat has been populated. # - If so - don't change it. # - If not, assume it hasn't been populated and copy the new Tomcat's ROOT over # - This needs to be updated to handle version updates of ROOT - name : Check ROOT exists stat : path : /var/lib/tomcat/ROOT register : root_webapps - name : Copy ROOT webapp directory copy : src : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/webapps/ROOT dest : /var/lib/tomcat/ remote_src : yes directory_mode : yes when : root_webapps.stat.exists == False ## remove directories (conf, logs, temp, webapps, work) and recreate as symlinks # conf - name : check {{ tomcat_ver }} conf directory stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/conf register : tomcat_conf_dir - name : Remove conf directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/conf state : absent when : tomcat_conf_dir.stat.isdir - name : Create conf symlink file : src : /etc/tomcat dest : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/conf owner : root group : tomcat state : link when : tomcat_conf_dir.stat.isdir or tomcat_conf_dir.stat.exists == False # logs - name : check {{ tomcat_ver }} logs directory stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/logs register : tomcat_logs_dir - name : Remove logs directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/logs state : absent when : tomcat_logs_dir.stat.isdir - name : Create logs symlink file : src : /var/log/tomcat dest : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/logs owner : tomcat group : tomcat state : link when : tomcat_logs_dir.stat.isdir or tomcat_logs_dir.stat.exists == False # temp - name : check {{ tomcat_ver }} temp directory stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/temp register : tomcat_temp_dir - name : Remove temp directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/temp state : absent when : tomcat_temp_dir.stat.isdir - name : Create temp symlink file : src : /var/cache/tomcat/temp dest : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/temp owner : tomcat group : tomcat state : link when : tomcat_temp_dir.stat.isdir or tomcat_temp_dir.stat.exists == False # webapps - name : check {{ tomcat_ver }} webapps directory stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/webapps register : tomcat_webapps_dir - name : Remove webapps directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/webapps state : absent when : tomcat_webapps_dir.stat.isdir - name : Create webapps symlink file : src : /var/lib/tomcat dest : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/webapps owner : root group : tomcat state : link when : tomcat_webapps_dir.stat.isdir or tomcat_webapps_dir.stat.exists == False # work - name : check {{ tomcat_ver }} work directory stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/work register : tomcat_work_dir - name : Remove webapps directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/work state : absent when : tomcat_work_dir.stat.isdir - name : Create work symlink file : src : /var/cache/tomcat/work dest : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/work owner : tomcat group : tomcat state : link when : tomcat_work_dir.stat.isdir or tomcat_webapps_dir.stat.exists == False # Setup systemd startup/shutdown script - name : Setup systemd startup/shutdown script template : src : tomcat.service.j2 dest : /etc/systemd/system/tomcat.service mode : 0644 owner : root group : root register : tomcat_service - name : Apply new SELinux file context to tomcat.service command : restorecon /etc/systemd/system/tomcat.service when : tomcat_service.changed - name : Reload systemd daemons after service update command : systemctl daemon-reload when : tomcat_service.changed","title":"Install Apache Tomcat"},{"location":"setting-up-the-environment/tomcat-ansible/tasks/#setup-tomcat-native-library","text":"This is where the Tomcat Native Library (if not already installed) gets unpacked, configured, compiled, and installed. - name : Check if Tomcat Native Library {{ tomcat_native_ver }} is already installed stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/lib/libtcnative-1.so register : tomcat_native_library # Untar tomcat native library - name : Unpack tomcat native library archive unarchive : src : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/tomcat-native.tar.gz\" dest : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/ owner : tomcat group : tomcat remote_src : yes when : tomcat_native_library.stat.exists == False # Only configure if the Tomcat Native Library directory for the version didn't already exist - name : Configure Tomcat Native Library (RHEL7 & JDK 11) command : \"./configure --with-java-home={{ JAVA_HOME }} --with-apr=/opt/apr/latest/bin/apr-1-config --prefix=/opt/tomcat/apache-tomcat-{{ tomcat_ver }}\" args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/tomcat-native-{{ tomcat_native_ver }}-src/native\" when : tomcat_native_library.stat.exists == False and ansible_distribution == 'RedHat' and ansible_distribution_major_version == '7' and jdk_version == 11 # Only configure if the Tomcat Native Library directory for the version didn't already exist - name : Configure Tomcat Native Library (RHEL7 & JDK 8) command : \"./configure --with-java-home={{ JAVA_8_HOME }} --with-apr=/opt/apr/latest/bin/apr-1-config --prefix=/opt/tomcat/apache-tomcat-{{ tomcat_ver }}\" args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/tomcat-native-{{ tomcat_native_ver }}-src/native\" when : tomcat_native_library.stat.exists == False and ansible_distribution == 'RedHat' and ansible_distribution_major_version == '7' and jdk_version == 8 # Only configure if the Tomcat Native Library directory for the version didn't already exist - name : Configure Tomcat Native Library (RHEL8) command : \"./configure --with-java-home={{ JAVA_HOME }} --with-apr=/opt/apr/latest/bin/apr-1-config --with-ssl=yes --prefix=/opt/tomcat/apache-tomcat-{{ tomcat_ver }}\" args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/tomcat-native-{{ tomcat_native_ver }}-src/native\" when : tomcat_native_library.stat.exists == False and ansible_distribution == 'RedHat' and ansible_distribution_major_version == '8' # Only make if the Tomcat Native Library directory for the version didn't already exist - name : Compile/install Tomcat Native Library shell : make && make install args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/tomcat-native-{{ tomcat_native_ver }}-src/native\" when : tomcat_native_library.stat.exists == False # Cleanup - name : Remove tomcat native source directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/tomcat-native-{{ tomcat_native_ver }}-src state : absent","title":"Setup Tomcat Native Library"},{"location":"setting-up-the-environment/tomcat-ansible/tasks/#setup-apache-tomcat-commons-daemon","text":"This is where the Tomcat Commons Daemon (if not already installed) gets unpacked, configured, compiled, and installed. - name : Check if Apache Tomcat Commons Daemon {{ commons_daemon_ver }} is already installed stat : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/jsvc register : commons_daemon_jsvc # Untar tomcat native library - name : Unpack Tomcat Commons Daemon native library unarchive : src : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/commons-daemon-native.tar.gz\" dest : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/\" owner : tomcat group : tomcat remote_src : yes when : commons_daemon_jsvc.stat.exists == False # Only configure if the Tomcat Native Library directory for the version didn't already exist - name : Configure Tomcat Commons Daemon native library (JDK 11) command : \"./configure --with-java={{ JAVA_HOME }}\" args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/commons-daemon-{{ commons_daemon_ver }}-native-src/unix\" when : commons_daemon_jsvc.stat.exists == False and jdk_version == 11 # Only configure if the Tomcat Native Library directory for the version didn't already exist - name : Configure Tomcat Commons Daemon native library (JDK 8) command : \"./configure --with-java={{ JAVA_8_HOME }}\" args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/commons-daemon-{{ commons_daemon_ver }}-native-src/unix\" when : commons_daemon_jsvc.stat.exists == False and jdk_version == 8 # Only make if the Tomcat Native Library directory for the version didn't already exist - name : Compile Tomcat Commons Daemon native library shell : make args : chdir : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/commons-daemon-{{ commons_daemon_ver }}-native-src/unix\" when : commons_daemon_jsvc.stat.exists == False - name : Move jsvc file copy : src : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/commons-daemon-{{ commons_daemon_ver }}-native-src/unix/jsvc\" dest : \"/opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/\" remote_src : yes owner : root group : tomcat mode : 0755 when : commons_daemon_jsvc.stat.exists == False # Cleanup - name : Remove commons-daemon source directory file : path : /opt/tomcat/apache-tomcat-{{ tomcat_ver }}/bin/commons-daemon-{{ commons_daemon_ver }}-native-src state : absent","title":"Setup Apache Tomcat Commons Daemon"},{"location":"setting-up-the-environment/tomcat-ansible/tasks/#configure-tomcat","text":"This is where we take the template files that were created earlier for web.xml, server.xml, etc., and make sure they are copied over to the server. If any are updated, it will notify Tomcat to restart when done. This is basically just configuring the various files in /etc/tomcat. To initially create these files - download copies of them from a default Tomcat install to your Ansible host's roles/apache-tomcat/templates directory and alter them as needed. I will have copies of these files uploaded as well.","title":"Configure Tomcat"},{"location":"setting-up-the-environment/tomcat-ansible/tasks/#to-do-add-links-to-the-documents-where-these-file-changes-are-done","text":"- name : Setup catalina.properties template : src : cas6-catalina.properties.j2 dest : /etc/tomcat/catalina.properties mode : 0640 owner : root group : tomcat when : (\"login\" in inventory_hostname) notify : restart tomcat - name : Setup context.xml template : src : cas6-context.xml.j2 dest : /etc/tomcat/context.xml mode : 0640 owner : root group : tomcat when : (\"login\" in inventory_hostname) notify : restart tomcat - name : Setup server.xml template : src : cas6-server.xml.j2 dest : /etc/tomcat/server.xml mode : 0640 owner : root group : tomcat when : (\"login\" in inventory_hostname) notify : restart tomcat - name : Setup web.xml template : src : cas6-web.xml.j2 dest : /etc/tomcat/web.xml mode : 0640 owner : root group : tomcat when : (\"login\" in inventory_hostname) notify : restart tomcat","title":"To do - add links to the documents where these file changes are done."},{"location":"setting-up-the-environment/tomcat-ansible/tasks/#post-installation-tasks","text":"If the installation fails (errors, not warnings) at any point - it won't get to here, so it shouldn't move the symlink and restart tomcat unless the install completed. It also won't 'notify' tomcat to restart if they didn't even have to create the symlink (or if the config files changed earlier) - name : Setup Apache Tomcat {{ tomcat_ver }} symlink file : src : /opt/tomcat/apache-tomcat-{{ tomcat_ver }} dest : /opt/tomcat/latest owner : root group : root state : link notify : restart tomcat # Cleanup - name : Remove tomcat tarball file : path : /root/apache-tomcat-{{ tomcat_ver }}.tar.gz state : absent","title":"Post installation tasks"},{"location":"setting-up-the-environment/tomcat-ansible/templates/","text":"Ansible Templates for Tomcat The following are the relative template files for Tomcat. They don't include the Apache HTTPD or CAS configs - this is JUST for Tomcat. When dealing with some things that have private keys or other sensitive info, you'll want to use Ansible vault. cas6-catalina.properties.j2 (destination is /etc/tomcat/context.xml) cas6-context.xml.j2 (destination is /etc/tomcat/context.xml) cas6-server.xml.j2 (destination is /etc/tomcat/server.xml) cas6-web.xml.j2 (destination is /etc/tomcat/web.xml) tomcat.service.j2 (destination is /etc/systemd/system/tomcat.service) Note: This doesn't have 'cas' in the file name since it is not CAS specific - it's the same for all our Tomcat installs. When I have a template specific to only one application/server/system - I indicate that in the file name.","title":"Setup Templates"},{"location":"setting-up-the-environment/tomcat-ansible/templates/#ansible-templates-for-tomcat","text":"The following are the relative template files for Tomcat. They don't include the Apache HTTPD or CAS configs - this is JUST for Tomcat. When dealing with some things that have private keys or other sensitive info, you'll want to use Ansible vault. cas6-catalina.properties.j2 (destination is /etc/tomcat/context.xml) cas6-context.xml.j2 (destination is /etc/tomcat/context.xml) cas6-server.xml.j2 (destination is /etc/tomcat/server.xml) cas6-web.xml.j2 (destination is /etc/tomcat/web.xml) tomcat.service.j2 (destination is /etc/systemd/system/tomcat.service) Note: This doesn't have 'cas' in the file name since it is not CAS specific - it's the same for all our Tomcat installs. When I have a template specific to only one application/server/system - I indicate that in the file name.","title":"Ansible Templates for Tomcat"},{"location":"setting-up-the-environment/tomcat-ansible/tomcat-ansible-overview/","text":"Using Ansible to put it all together Okay - now that I've covered all the steps that are done to get Tomcat working - you should realize I never do them manually anymore. I have way too much to do to be manually doing all that. When I initially started doing this for CAS 5, I had a combination of shell and python scripts to streamline this - but as I've gotten to use Ansible more over the past few years, I moved it all there. Ansible hosts file You have to have an Ansible hosts file created. You can start with a very small config for a single server even: [test_phase2] login6devb ansible_python_interpreter=/usr/libexec/platform-python tomcat_major_ver=9.0 tomcat_ver=9.0.41 jdk_version=11 What I've defined here is: ansible_python_interpreter (since RHEL 8 doesn't come with a default python in the path, you have to tell Ansible where it can find python) tomcat_major_ver (since some of my Ansible tasks are version specific, as I still have some Tomcat 8.5.x systems) tomcat_ver (this is where the specific version of Tomcat is specified. This is helpful when you want to push out updates to your test/dev systems first, but aren't ready to for production) jdk_version (likewise, I have systems which are using OpenJDK 8 and others with OpenJDK 11, and I want to have my plays handle these.) Define a site.yml file This is where you can associate roles with systems. As an example, here's my development CAS hosts, where I have three separate roles (one for security hardening, based on the Center for Internet Security benchmarks, one for Apache Tomcat, and one for Apache httpd. It should be within your main ansible directory --- # CAS 6 - hosts : login6deva,login6devb roles : - security-hardening-rhel8 - apache-tomcat - apache-httpd Create a role for Tomcat If you don't already have a 'roles' directory within your main ansible directory, create one. Mine sits within my own user directory on my Ansible/build server. Once that is created, go to the roles directory and initialize a new role for apache-tomcat: [chauvetp@ansible ~]$ cd ansible/roles/ [chauvetp@ansible roles]$ ansible-galaxy init apache-tomcat - Role apache-tomcat was created successfully [chauvetp@ansible roles]$ ls apache-tomcat/ defaults files handlers meta README.md tasks templates tests vars","title":"Ansible and Tomcat - Initial Steps"},{"location":"setting-up-the-environment/tomcat-ansible/tomcat-ansible-overview/#using-ansible-to-put-it-all-together","text":"Okay - now that I've covered all the steps that are done to get Tomcat working - you should realize I never do them manually anymore. I have way too much to do to be manually doing all that. When I initially started doing this for CAS 5, I had a combination of shell and python scripts to streamline this - but as I've gotten to use Ansible more over the past few years, I moved it all there.","title":"Using Ansible to put it all together"},{"location":"setting-up-the-environment/tomcat-ansible/tomcat-ansible-overview/#ansible-hosts-file","text":"You have to have an Ansible hosts file created. You can start with a very small config for a single server even: [test_phase2] login6devb ansible_python_interpreter=/usr/libexec/platform-python tomcat_major_ver=9.0 tomcat_ver=9.0.41 jdk_version=11 What I've defined here is: ansible_python_interpreter (since RHEL 8 doesn't come with a default python in the path, you have to tell Ansible where it can find python) tomcat_major_ver (since some of my Ansible tasks are version specific, as I still have some Tomcat 8.5.x systems) tomcat_ver (this is where the specific version of Tomcat is specified. This is helpful when you want to push out updates to your test/dev systems first, but aren't ready to for production) jdk_version (likewise, I have systems which are using OpenJDK 8 and others with OpenJDK 11, and I want to have my plays handle these.)","title":"Ansible hosts file"},{"location":"setting-up-the-environment/tomcat-ansible/tomcat-ansible-overview/#define-a-siteyml-file","text":"This is where you can associate roles with systems. As an example, here's my development CAS hosts, where I have three separate roles (one for security hardening, based on the Center for Internet Security benchmarks, one for Apache Tomcat, and one for Apache httpd. It should be within your main ansible directory --- # CAS 6 - hosts : login6deva,login6devb roles : - security-hardening-rhel8 - apache-tomcat - apache-httpd","title":"Define a site.yml file"},{"location":"setting-up-the-environment/tomcat-ansible/tomcat-ansible-overview/#create-a-role-for-tomcat","text":"If you don't already have a 'roles' directory within your main ansible directory, create one. Mine sits within my own user directory on my Ansible/build server. Once that is created, go to the roles directory and initialize a new role for apache-tomcat: [chauvetp@ansible ~]$ cd ansible/roles/ [chauvetp@ansible roles]$ ansible-galaxy init apache-tomcat - Role apache-tomcat was created successfully [chauvetp@ansible roles]$ ls apache-tomcat/ defaults files handlers meta README.md tasks templates tests vars","title":"Create a role for Tomcat"},{"location":"setting-up-the-environment/tomcat-ansible/variables/","text":"Ansible Variable setup for Tomcat I don't have a lot of variables for Tomcat on it's own, and none are 'sensitive'. They're basically version information so I don't have to hard code versions into the plays. If you ever have sensitive info that needs to go into a variables file, you'll want to use Ansible Vault so it isn't plain text. # where the Tomcat archive file gets downloaded tomcat_archive_dest : /root/apache-tomcat-{{ tomcat_ver }}.tar.gz # Apache Portable Runtime version - it's been 1.7.0 since April 2019 # Your actual mirror will vary - I didn't programatically pick something random # So you may not want to just copy the mirror for APR I have here. apr_ver : 1.7.0 apr_archive_url : http://apache.cs.utah.edu//apr/apr-{{ apr_ver }}.tar.gz apr_archive_dest : /root/apr-{{ apr_ver }}.tar.gz # Legacy - for RHEL7/CAS5 servers - this is not needed for CAS 6. openssl_ver : 1.1.1j openssl_archive_url : https://www.openssl.org/source/openssl-{{ openssl_ver }}.tar.gz openssl_archive_dest : /root/openssl-{{ openssl_ver }}.tar.gz # Unfortunately - without reading release notes, or unpacking Tomcat to test when a new version comes out, # you won't know what version of the Commons Daemon or Tomcat Native Library are in that Tomcat version. # My practice is when upgrading to a new version for the first time - I will download that version, # unpack it, and unpack the Tomcat Native Library and Commons Daemon to check their version. tomcat_native_ver : 1.2.25 commons_daemon_ver : 1.2.3 JAVA_HOME : /usr/lib/jvm/java-11-openjdk # Legacy - for some older systems - not for CAS 6. JAVA_8_HOME : /usr/lib/jvm/java-1.8.0","title":"Setup Variables"},{"location":"setting-up-the-environment/tomcat-ansible/variables/#ansible-variable-setup-for-tomcat","text":"I don't have a lot of variables for Tomcat on it's own, and none are 'sensitive'. They're basically version information so I don't have to hard code versions into the plays. If you ever have sensitive info that needs to go into a variables file, you'll want to use Ansible Vault so it isn't plain text. # where the Tomcat archive file gets downloaded tomcat_archive_dest : /root/apache-tomcat-{{ tomcat_ver }}.tar.gz # Apache Portable Runtime version - it's been 1.7.0 since April 2019 # Your actual mirror will vary - I didn't programatically pick something random # So you may not want to just copy the mirror for APR I have here. apr_ver : 1.7.0 apr_archive_url : http://apache.cs.utah.edu//apr/apr-{{ apr_ver }}.tar.gz apr_archive_dest : /root/apr-{{ apr_ver }}.tar.gz # Legacy - for RHEL7/CAS5 servers - this is not needed for CAS 6. openssl_ver : 1.1.1j openssl_archive_url : https://www.openssl.org/source/openssl-{{ openssl_ver }}.tar.gz openssl_archive_dest : /root/openssl-{{ openssl_ver }}.tar.gz # Unfortunately - without reading release notes, or unpacking Tomcat to test when a new version comes out, # you won't know what version of the Commons Daemon or Tomcat Native Library are in that Tomcat version. # My practice is when upgrading to a new version for the first time - I will download that version, # unpack it, and unpack the Tomcat Native Library and Commons Daemon to check their version. tomcat_native_ver : 1.2.25 commons_daemon_ver : 1.2.3 JAVA_HOME : /usr/lib/jvm/java-11-openjdk # Legacy - for some older systems - not for CAS 6. JAVA_8_HOME : /usr/lib/jvm/java-1.8.0","title":"Ansible Variable setup for Tomcat"}]}